{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d449a5-2c3e-43f5-8d18-25f53bee7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import argparse\n",
    "import numpy as np\n",
    "from torch_geometric.nn import conv\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_curve, average_precision_score\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict as ddict, Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfff87f-32ec-4187-918f-f803b3f7aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= argparse.ArgumentParser(description= 'Parser for Arguments')\n",
    "parser.add_argument('-seed', type= int, default= 0)\n",
    "parser.add_argument('-num_ent', type= int, default= 1209+ 172+ 154)\n",
    "parser.add_argument('-num_drug', type= int, default= 1209)\n",
    "parser.add_argument('-num_micr', type= int, default= 172)\n",
    "parser.add_argument('-num_dise', type= int, default= 154)\n",
    "# drug_micr_rel, 0; drug_dise_rel, 1; micr_dise_rel, 2; drug_inter_rel, 3; micr_inter_rel, 4; micr_drug_rel, 5; dise_drug_rel, 6; dies_micr_rel, 7.\n",
    "parser.add_argument('-num_rel', type= int, default= 8)\n",
    "parser.add_argument('-drug_name_path', type= str, default= '../mdd/drug/drug_name.txt')\n",
    "parser.add_argument('-micr_name_path', type= str, default= '../mdd/microbe/microbe_name.txt')\n",
    "parser.add_argument('-dise_name_path', type= str, default= '../mdd/disease/disease_name.txt')\n",
    "parser.add_argument('-drug_micr_adj_path', type= str, default= '../mdd/adj/microbe_drug_adj.txt')\n",
    "parser.add_argument('-drug_struct_simi_path', type= str, default= '../mdd/drug/drug_struct_simi.txt')\n",
    "parser.add_argument('-drug_inter_adj_path', type= str, default= '../mdd/drug/drug_interact_adj.txt')\n",
    "parser.add_argument('-drug_fringer_simi_path', type= str, default= '../mdd/drug/drug_fringer_simi.txt')\n",
    "parser.add_argument('-drug_dise_adj_path', type= str, default= '../mdd/adj/drug_disease_adj.txt')\n",
    "parser.add_argument('-micr_ani_path', type= str, default= '../mdd/microbe/microbe_ani_simi.txt')\n",
    "parser.add_argument('-micr_inter_adj_path', type= str, default= '../mdd/microbe/microbe_interact_adj.txt')\n",
    "parser.add_argument('-micr_dise_adj_path', type= str, default= '../mdd/adj/microbe_disease_adj.txt')\n",
    "parser.add_argument('-dise_simi_path', type= str, default= '../mdd/disease/disease_dag_simi.txt')\n",
    "parser.add_argument('-train_ratio', type= float, default= 0.8)\n",
    "parser.add_argument('-valid_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-test_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-kg_file', type= str, default= 'kg_data/')\n",
    "parser.add_argument('-batch_size', type= int, default= 128)\n",
    "parser.add_argument('-lbl_smooth', type= float, default= 0.2, help= 'Label smoothing enable or disable')\n",
    "parser.add_argument('-embed_dim', type= int, default= 128)\n",
    "parser.add_argument('-device', type= str, default= 'cuda:0')\n",
    "parser.add_argument('-lr_kg', type= float, default= 0.001)\n",
    "parser.add_argument('-weight_decay_kg', type= float, default= 0)\n",
    "parser.add_argument('-patience_kg', type= int, default= 50)\n",
    "parser.add_argument('-epoch_kg', type= int, default= 300)\n",
    "parser.add_argument('-pt_file', type= str, default= 'checkpoint/')\n",
    "parser.add_argument('-memo_file4kg', type= str, default= 'memo/memo.txt')\n",
    "parser.add_argument('-threshold4kg', type= float, default= 0.8)\n",
    "parser.add_argument('-pt_file_name4net1', type= str, default= 'conve.pt')\n",
    "parser.add_argument('-test_result_file', type= str, default= 'result/conve_test_result.txt')\n",
    "params= parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccbd45-2a47-499a-bc99-68f2f256e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed_all(params.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(params.seed)    \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a5880-53d8-4520-9b61-39f4b2a03be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(object):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params= params\n",
    "        self.drug_micr_asso_mat, self.drug_dise_asso_mat, self.drug_inter_mat, self.drug_struct_simi_mat, self.drug_fringer_simi_mat= self.load_drug_data()\n",
    "        self.train_xy, self.valid_xy, self.test_xy= self.split_dataset()\n",
    "        self.drug_micr_asso_mat_zy= self.get_asso_mat_zy()\n",
    "        self.micr_ani_mat, self.micr_inter_mat, self.micr_dise_asso_mat, self.micr_asso_simi_mat= self.load_micr_data()\n",
    "        self.dise_simi_mat, self.drug_dise_drug_simi_mat, self.micr_dise_micr_simi_mat= self.load_dise_data()\n",
    "        self.hete_graph_mat= torch.cat([torch.cat([self.drug_struct_simi_mat, self.drug_micr_asso_mat_zy, self.drug_dise_asso_mat], dim= 1),\\\n",
    "                                    torch.cat([self.drug_micr_asso_mat_zy.T, self.micr_ani_mat, self.micr_dise_asso_mat], dim= 1),\\\n",
    "                                    torch.cat([self.drug_dise_asso_mat.T, self.micr_dise_asso_mat.T, self.dise_simi_mat], dim= 1)], dim= 0).float()       \n",
    "        self.create_kg_data()\n",
    "        self.introduce()\n",
    "\n",
    "    # @introduce data\n",
    "    def introduce(self):\n",
    "        print(f'Drug microbe association num: {self.drug_micr_asso_mat.sum()}\\nDrug interaction num: {self.drug_inter_mat.sum()}\\nMicrobe interaction num: {self.micr_inter_mat.sum()}')\n",
    "        print(f'Drug disease association num: {self.drug_dise_asso_mat.sum()}\\nMicrobe disease association num: {self.micr_dise_asso_mat.sum()}')\n",
    "\n",
    "    # @ mask\n",
    "    def get_asso_mat_zy(self):\n",
    "        asso_mat_zy= self.drug_micr_asso_mat.clone()\n",
    "        asso_mat_zy[self.valid_xy[:, 0], self.valid_xy[:, 1]]= 0\n",
    "        asso_mat_zy[self.test_xy[:, 0], self.test_xy[:, 1]]= 0\n",
    "        return asso_mat_zy\n",
    "    \n",
    "    # @create knowledge graph data\n",
    "    def create_kg_data(self):\n",
    "        if os.path.exists(self.params.kg_file)== False:os.makedirs(self.params.kg_file)\n",
    "        # drug microbe association data\n",
    "        drug_micr_train, drug_micr_valid, drug_micr_test= self.train_xy[self.drug_micr_asso_mat[self.train_xy[:, 0], self.train_xy[:, 1]]== 1],\\\n",
    "        self.valid_xy[self.drug_micr_asso_mat[self.valid_xy[:, 0], self.valid_xy[:, 1]]== 1],\\\n",
    "        self.test_xy[self.drug_micr_asso_mat[self.test_xy[:, 0], self.test_xy[:, 1]]== 1]\n",
    "        # drug disease association data\n",
    "        drug_dise_train, micr_dise_train, drug_inter_train, micr_inter_train= self.drug_dise_asso_mat.nonzero(), self.micr_dise_asso_mat.nonzero(), self.drug_inter_mat.nonzero(), self.micr_inter_mat.nonzero()\n",
    "        # add offset\n",
    "        drug_micr_valid+= torch.tensor([0, self.params.num_drug]); drug_micr_test+= torch.tensor([0, self.params.num_drug]); drug_micr_train+= torch.tensor([0, self.params.num_drug])\n",
    "        drug_dise_train+= torch.tensor([0, self.params.num_drug+ self.params.num_micr]); micr_dise_train+= torch.tensor([self.params.num_drug, self.params.num_drug+ self.params.num_micr]); micr_inter_train+= torch.tensor([self.params.num_drug, self.params.num_drug])\n",
    "        # add rel\n",
    "        drug_micr_train, drug_micr_valid, drug_micr_test= drug_micr_train[:, [0, 1, 1]], drug_micr_valid[:, [0, 1, 1]], drug_micr_test[:, [0, 1, 1]]\n",
    "        drug_dise_train, micr_dise_train, drug_inter_train, micr_inter_train= drug_dise_train[:, [0, 1, 1]], micr_dise_train[:, [0, 1, 1]], drug_inter_train[:, [0, 1, 1]], micr_inter_train[:, [0, 1, 1]]\n",
    "        drug_micr_train[:, 2], drug_micr_valid[:, 2], drug_micr_test[:, 2], drug_dise_train[:, 2], micr_dise_train[:, 2], drug_inter_train[:, 2], micr_inter_train[:, 2]= 0, 0, 0, 1, 2, 3, 4\n",
    "        # savefile\n",
    "        train= torch.cat([drug_micr_train, drug_dise_train, micr_dise_train, drug_inter_train, micr_inter_train], dim= 0)\n",
    "        val= drug_micr_valid\n",
    "        test= drug_micr_test\n",
    "        np.savetxt(f'{params.kg_file}//train.txt', train, fmt= '%d', delimiter= '\\t', encoding= 'utf-8-sig')\n",
    "        np.savetxt(f'{params.kg_file}//valid.txt', val, fmt= '%d', delimiter= '\\t', encoding= 'utf-8-sig')\n",
    "        np.savetxt(f'{params.kg_file}//test.txt', test, fmt= '%d', delimiter= '\\t', encoding= 'utf-8-sig')\n",
    "        print(f'Knowledge graph data has prepared...')\n",
    "\n",
    "    # @split data set\n",
    "    def split_dataset(self):\n",
    "        train_xy, valid_xy, test_xy= [], [], []\n",
    "        for i in range(self.params.num_drug):\n",
    "            first= True\n",
    "            for j in range(self.params.num_micr):\n",
    "                if self.drug_micr_asso_mat[i, j]== 1 and first:\n",
    "                    train_xy.append([i, j])\n",
    "                    first= False\n",
    "                else:\n",
    "                    num= torch.rand(1)\n",
    "                    if num< self.params.train_ratio:\n",
    "                        train_xy.append([i, j])\n",
    "                    elif num>= self.params.train_ratio and num< self.params.train_ratio+ self.params.valid_ratio:\n",
    "                        valid_xy.append([i, j])\n",
    "                    else:\n",
    "                        test_xy.append([i, j])        \n",
    "        print(f'Spliting data has finished...')\n",
    "        return torch.tensor(train_xy), torch.tensor(valid_xy), torch.tensor(test_xy)\n",
    "\n",
    "    # @load disease data\n",
    "    def load_dise_data(self):\n",
    "        dise_simi_mat= torch.from_numpy(np.loadtxt(self.params.dise_simi_path, encoding= 'utf-8-sig'))\n",
    "        drug_dise_drug_simi_mat, micr_dise_micr_simi_mat= torch.matmul(nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1).T),\\\n",
    "        torch.matmul(nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1).T)\n",
    "        for i in range(self.params.num_drug): drug_dise_drug_simi_mat[i, i]= 1.0\n",
    "        for i in range(self.params.num_micr): micr_dise_micr_simi_mat[i, i]= 1.0\n",
    "        return dise_simi_mat, drug_dise_drug_simi_mat, micr_dise_micr_simi_mat\n",
    "\n",
    "    # @load micr data\n",
    "    def load_micr_data(self):\n",
    "        micr_ani_mat= torch.from_numpy(np.loadtxt(self.params.micr_ani_path, encoding= 'utf-8-sig'))\n",
    "        micr_inter_mat= self.load_adj_data(self.params.micr_inter_adj_path, sp= (self.params.num_micr, self.params.num_micr))\n",
    "        micr_dise_mat= self.load_adj_data(self.params.micr_dise_adj_path, sp= (self.params.num_micr, self.params.num_dise))\n",
    "        micr_asso_simi_mat= torch.matmul(nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1), nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1).T)\n",
    "        for i in range(self.params.num_micr):micr_asso_simi_mat[i, i]= 1\n",
    "        for i in range(self.params.num_micr):micr_ani_mat[i, i]= 1            \n",
    "        return micr_ani_mat, micr_inter_mat, micr_dise_mat, micr_asso_simi_mat\n",
    "\n",
    "    # @load drug data\n",
    "    def load_drug_data(self):\n",
    "        drug_dise_asso_mat= self.load_adj_data(self.params.drug_dise_adj_path, sp= (self.params.num_drug, self.params.num_dise))\n",
    "        drug_micr_asso_mat= self.load_adj_data(self.params.drug_micr_adj_path, sp= (self.params.num_drug, self.params.num_micr))\n",
    "        drug_inter_mat= self.load_adj_data(self.params.drug_inter_adj_path, sp= (self.params.num_drug, self.params.num_drug))\n",
    "        drug_struct_simi_mat= torch.from_numpy(np.loadtxt(self.params.drug_struct_simi_path, encoding= 'utf-8-sig'))\n",
    "        drug_fringer_simi_mat= torch.from_numpy(np.loadtxt(self.params.drug_fringer_simi_path, encoding= 'utf-8-sig'))\n",
    "        return drug_micr_asso_mat, drug_dise_asso_mat, drug_inter_mat, drug_struct_simi_mat, drug_fringer_simi_mat\n",
    "    \n",
    "    # @load adj data\n",
    "    def load_adj_data(self, path, sp= (1209, 172)):\n",
    "        idx= torch.from_numpy(np.loadtxt(path, encoding= 'utf-8-sig')).long()- 1\n",
    "        mat= torch.zeros((sp[0], sp[1]))\n",
    "        mat[idx[:, 0], idx[:, 1]]= 1\n",
    "        return mat\n",
    "    \n",
    "    # write into memo\n",
    "    def write2memo(self, mr, mrr, hits10):\n",
    "        with open(f'{self.params.memo_file4kg}', 'a+') as f:\n",
    "            f.write(f'{self.params.lr_kg}\\t{self.params.weight_decay_kg}\\t{mr}\\t{mrr}\\t{hits10}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a11f7df-8db9-4c31-846a-f97939098bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl= dataloader(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b139b0-9975-4ed4-a9a2-1683a9a041a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\t\"\"\"docstring for EarlyStopping\"\"\"\n",
    "\tdef __init__(self, patience, pt_file= 'checkpoint/', file_name= 'checkpoint.pt', mess_out= True, eps= 0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.patience, self.eps, self.pt_file, self.file_name, self.mess_out= patience, eps, pt_file, file_name, mess_out\n",
    "\t\tself.best_score, self.counter, self.flag= None, 0, False\n",
    "\t\tif os.path.exists(self.pt_file)== False:os.makedirs(self.pt_file)\n",
    "\t\n",
    "\tdef __call__(self, val_loss, model):\n",
    "\t\tscore= -val_loss\n",
    "\t\tif self.best_score is None:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\telif score< self.best_score- self.eps:\n",
    "\t\t\tself.counter+= 1\n",
    "\t\t\tif self.mess_out:print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "\t\t\tif self.counter>= self.patience:\n",
    "\t\t\t\tself.flag= True\n",
    "\t\telse:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\t\tself.counter= 0\n",
    "\n",
    "\tdef save_checkpoint(self, model):\n",
    "\t\ttorch.save(model, f'{self.pt_file}//{self.file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a605d8-4e7f-4ecb-b36e-071b265c85fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, triples, split, params):\n",
    "        self.triples= triples\n",
    "        self.split= split\n",
    "        self.params= params\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ele= self.triples[idx]\n",
    "        triple, label= torch.LongTensor(ele['triple']), np.int32(ele['label'])\n",
    "        label= self.get_label(label)\n",
    "        if self.split== 'train' and self.params.lbl_smooth!= 0.0:\n",
    "            label= (1.0- self.params.lbl_smooth)* label+ (1.0/ self.params.num_ent)\n",
    "        return triple, label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        triple= torch.stack([_[0] for _ in data], dim= 0)\n",
    "        label= torch.stack([_[1] for _ in data], dim= 0)\n",
    "        return triple, label\n",
    "\n",
    "    def get_label(self, label):\n",
    "        y= np.zeros([self.params.num_ent], dtype= np.float32)\n",
    "        for e2 in label: y[e2]= 1.0\n",
    "        return torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c70b1-f618-4bad-ab5a-e345d8b6107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(params):\n",
    "\n",
    "    sr2d, data, tp= ddict(set), ddict(list), 0\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for line in open(f'{params.kg_file}{split}.txt', encoding= 'utf-8-sig'):\n",
    "            src_id, dst_id, rel_id= line.strip().split('\\t')\n",
    "            src_id, rel_id, dst_id= int(float(src_id)), int(float(rel_id)), int(float(dst_id))\n",
    "            data[split].append((src_id, rel_id, dst_id))\n",
    "            if split== 'train':\n",
    "                sr2d[(src_id, rel_id)].add(dst_id)\n",
    "                if rel_id in [0, 1, 2]:sr2d[(dst_id, rel_id+ 5)].add(src_id)\n",
    "    sr2d4tr= {k: list(v) for k, v in sr2d.items()}\n",
    "    triples= ddict(list)\n",
    "    for (src_id, rel_id), dst_id in sr2d4tr.items():\n",
    "        triples['train'].append({'triple': (src_id, rel_id, -1), 'label': dst_id})\n",
    "    for split in ['test', 'valid']:\n",
    "        for src_id, rel_id, dst_id in data[split]:\n",
    "            sr2d[(src_id, rel_id)].add(dst_id)\n",
    "            sr2d[(dst_id, rel_id+ 5)].add(src_id)\n",
    "    sr2d4val_te= {k: list(v) for k, v in sr2d.items()}\n",
    "    for split in ['valid', 'test']:\n",
    "        for src_id, rel_id, dst_id in data[split]:\n",
    "            triples[f'{split}'].append({'triple': (src_id, rel_id, dst_id), 'label': sr2d4val_te[(src_id, rel_id)]})\n",
    "    triples= dict(triples)\n",
    "    def get_data_loader(dataset_class, split, batch_size, shuffle= True):\n",
    "        return DataLoader(dataset_class(triples[split], split, params), batch_size= batch_size, shuffle= shuffle, collate_fn= dataset_class.collate_fn)\n",
    "    data_iter= {\n",
    "        'train': get_data_loader(MDDataset, 'train', params.batch_size),\n",
    "        'valid': get_data_loader(MDDataset, 'valid', params.batch_size),\n",
    "        'test': get_data_loader(MDDataset, 'test', params.batch_size)}\n",
    "    return data_iter, triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32823ffd-5b58-4530-85ec-77bc4017de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter, triples= load_data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d32c2-9a83-448e-83af-5e837adc6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvE(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params= params\n",
    "        self.ent_embed= torch.nn.Embedding(self.params.num_ent, self.params.embed_dim, padding_idx= None); nn.init.xavier_normal_(self.ent_embed.weight)\n",
    "        self.rel_embed= torch.nn.Embedding(self.params.num_rel, self.params.embed_dim, padding_idx= None); nn.init.xavier_normal_(self.rel_embed.weight)\n",
    "        self.input_drop, self.feature_drop, self.hidden_drop= torch.nn.Dropout(0.3), torch.nn.Dropout2d(0.3), torch.nn.Dropout(0.3)\n",
    "        self.bn0, self.bn1, self.bn2= torch.nn.BatchNorm2d(1), torch.nn.BatchNorm2d(32), torch.nn.BatchNorm1d(self.params.embed_dim)\n",
    "        self.cv1= nn.Conv2d(1, out_channels= 32, kernel_size= (3, 3), stride= 1, padding= 0, bias= True)\n",
    "        self.fc= nn.Sequential(nn.Linear(32* 14* 14, 2* self.params.embed_dim), nn.ReLU(), nn.Dropout(0.3), nn.Linear(2* self.params.embed_dim, self.params.embed_dim));nn.init.xavier_normal_(self.fc[0].weight, nn.init.calculate_gain('relu'));nn.init.xavier_normal_(self.fc[3].weight)\n",
    "        self.act1, self.act2= nn.ReLU(), nn.Sigmoid()\n",
    "        self.register_parameter('bias', nn.Parameter(torch.zeros(self.params.num_ent)))        \n",
    "        self.loss_fuc= torch.nn.BCELoss()\n",
    "        \n",
    "    def loss(self, pred, true_label):\n",
    "        return self.loss_fuc(pred, true_label)\n",
    "        \n",
    "    def forward(self, src, rel):\n",
    "        semb, remb= self.ent_embed(src).view(-1, 1, 8, 16), self.rel_embed(rel).view(-1, 1, 8, 16)\n",
    "        # (128, 1, 16, 16)\n",
    "        emb= torch.cat([semb, remb], dim= 2)\n",
    "        x= self.input_drop(self.bn0(emb))\n",
    "        # (128, 32, 14, 14)\n",
    "        x= self.feature_drop(self.act1(self.bn1(self.cv1(x))))\n",
    "        x= self.act1(self.bn2(self.hidden_drop(self.fc(x.view(src.shape[0], -1)))))\n",
    "        x= torch.mm(x, self.ent_embed.weight.transpose(1, 0))\n",
    "        x+= self.bias.expand_as(x)\n",
    "        pred= self.act2(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82371b7-1cfa-41f6-929c-d8b3b4a43a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, data_iter, params, split= 'valid'):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        results= {}\n",
    "        train_iter= iter(data_iter[split])\n",
    "        for step, batch in enumerate(train_iter):\n",
    "            triple, label= [_.to(params.device) for _ in batch]\n",
    "            src, rel, dst, label= triple[:, 0], triple[:, 1], triple[:, 2], label\n",
    "            pred= net.forward(src, rel)\n",
    "            b_range= torch.arange(pred.size()[0], device= params.device)\n",
    "            target_pred= pred[b_range, dst]\n",
    "            pred= torch.where(label.byte(), torch.zeros_like(pred), pred)\n",
    "            pred[b_range, dst]= target_pred\n",
    "            pred= pred.cpu().numpy()\n",
    "            dst= dst.cpu().numpy()\n",
    "            for i in range(pred.shape[0]):\n",
    "                scores= pred[i]\n",
    "                target= dst[i]\n",
    "                tar_scr= scores[target]\n",
    "                scores= np.delete(scores, target)\n",
    "                rand= np.random.randint(scores.shape[0])\n",
    "                scores= np.insert(scores, rand, tar_scr)\n",
    "                sorted_indices= np.argsort(-scores, kind= 'stable')\n",
    "                _filter= np.where(sorted_indices== rand)[0][0]\n",
    "                results['count']= 1+ results.get('count', 0.0)\n",
    "                results['mr']= (_filter+ 1)+ results.get('mr', 0.0)\n",
    "                results['mrr']= (1.0/ (_filter+ 1))+ results.get('mrr', 0.0)\n",
    "                for k in range(10):\n",
    "                    if _filter<= k:\n",
    "                        results[f'hits@{k+ 1}']= 1+ results.get(f'hits@{k+ 1}', 0.0)\n",
    "    results['mr']= round(results['mr']/ float(results['count']), 5)\n",
    "    results['mrr']= round(results['mrr']/ float(results['count']), 5)\n",
    "    for k in range(10):\n",
    "        results[f'hits@{k+1}']= round(results.get(f'hits@{k+ 1}', 0)/ float(results['count']), 5)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2b54f-2302-4316-a5aa-bae027e2fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "net= ConvE(params).to(params.device)\n",
    "optimizer= torch.optim.Adam(net.parameters(), lr= params.lr_kg, weight_decay= params.weight_decay_kg)\n",
    "earlystopping4kg= EarlyStopping(patience= params.patience_kg, pt_file= params.pt_file, file_name= params.pt_file_name4net1, mess_out= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732556bc-812b-45d4-bb6c-99578bc091ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(params.epoch_kg):\n",
    "    # train\n",
    "    net.train()\n",
    "    losses= []\n",
    "    train_iter= iter(data_iter['train'])\n",
    "    for step, batch in enumerate(train_iter):\n",
    "        optimizer.zero_grad()\n",
    "        triple, label= [_.to(params.device) for _ in batch]\n",
    "        src, rel, dst, label= triple[:, 0], triple[:, 1], triple[:, 2], label\n",
    "        pred= net.forward(src, rel)\n",
    "        loss= net.loss(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if step% 5000== 0:\n",
    "            print(f'epoch: {epoch}, Train loss: {np.mean(losses)}, rel ratio: {((rel== 0)).sum()/ len(rel)}, acc: {(torch.where(pred< 0.5, 0, 1)== torch.where(label< 0.5, 0, 1)).sum()/ label.shape[0]/ label.shape[1]}')\n",
    "    # valid\n",
    "    results= evaluate(net, data_iter, params, 'valid')\n",
    "    print(f'val mr: {results[\"mr\"]}, mrr: {results[\"mrr\"]}, hits10: {results[\"hits@10\"]}')\n",
    "    earlystopping4kg(-(results['mrr']+ results['hits@10']), net)\n",
    "    if earlystopping4kg.flag== True: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b8883-96e4-4051-9d50-cd36ad4ea7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "net= torch.load(f'{params.pt_file}//{params.pt_file_name4net1}')\n",
    "valid_results= evaluate(net, data_iter, params, 'valid')\n",
    "print(f'valid results, {valid_results}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf565bf4-86c8-44dd-a43e-5034a3e414b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_emb, rel_emb= net.ent_embed.weight, net.rel_embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52295c8-515e-4898-bcb0-76cd90d33903",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label, valid_label, test_label= dl.drug_micr_asso_mat[dl.train_xy[:, 0], dl.train_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.valid_xy[:, 0], dl.valid_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.test_xy[:, 0], dl.test_xy[:, 1]].long()\n",
    "# train\n",
    "train_xy_label_dataset= torch.utils.data.TensorDataset(dl.train_xy, train_label)\n",
    "train_loader= torch.utils.data.DataLoader(train_xy_label_dataset, batch_size= params.batch_size, shuffle= False)\n",
    "# valid\n",
    "valid_xy_label_dataset= torch.utils.data.TensorDataset(dl.valid_xy, valid_label)\n",
    "valid_loader= torch.utils.data.DataLoader(valid_xy_label_dataset, batch_size= params.batch_size, shuffle= False)\n",
    "# test, \n",
    "test_xy_label_dataset= torch.utils.data.TensorDataset(dl.test_xy, test_label)\n",
    "test_loader= torch.utils.data.DataLoader(test_xy_label_dataset, batch_size= params.batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1108609-b387-4646-8a18-696529242033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_auc_aupr_cpt(test_xy, test_label, pred, ass_mat_shape):\n",
    "    label_mat, pred_mat= torch.zeros((ass_mat_shape)) -1, torch.zeros((ass_mat_shape)) -1\n",
    "    label_mat[test_xy[:, 0], test_xy[:, 1]], pred_mat[test_xy[:, 0], test_xy[:, 1]]= test_label* 1.0, pred\n",
    "    bool_mat4mark_test_examp= (label_mat!= -1)\n",
    "    aucs, auprs= [], []\n",
    "    for i in range(ass_mat_shape[0]):\n",
    "        test_examp_loc= bool_mat4mark_test_examp[i]\n",
    "        pos_num= label_mat[i, test_examp_loc].sum()\n",
    "        if pos_num> 0 and (test_examp_loc).sum()- pos_num> 0:\n",
    "            fpr4rowi, tpr4rowi, _= roc_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi, recall4rowi, _= precision_recall_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi[-1]= [1, 0][(int)(prec4rowi[-2]== 0)]\n",
    "            aucs.append(auc(fpr4rowi, tpr4rowi));auprs.append(auc(recall4rowi, prec4rowi))\n",
    "    return np.mean(aucs), np.mean(auprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0c299-5918-4af7-acb1-c3f845bb2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= net(torch.arange(0, 1535, 1).to(torch.long).to(params.device), torch.zeros(1535).to(torch.long).to(params.device))[:, 1209: 1209+ 172].detach().clone().cpu()\n",
    "pred= pred[dl.test_xy[:, 0], dl.test_xy[:, 1]]\n",
    "results= torch.cat([dl.test_xy.to('cpu'), test_label.view(-1, 1).to('cpu'), pred.view(-1, 1).to('cpu')], dim= 1)\n",
    "print(avg_auc_aupr_cpt(dl.test_xy.to('cpu'), test_label.to('cpu'), pred.to('cpu'), (1209, 172)))\n",
    "# np.savetxt(fname= params.test_result_file, X= results, delimiter= '\\t', encoding= 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
