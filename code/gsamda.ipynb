{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a0a8d-4bbe-44c9-8d64-6725dafe694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b55c25-647e-4943-b722-41ac0c4fb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= argparse.ArgumentParser(description= 'Parser for Arguments')\n",
    "parser.add_argument('-seed', type= int, default= 0)\n",
    "parser.add_argument('-num_ent', type= int, default= 1209+ 172+ 154)\n",
    "parser.add_argument('-num_drug', type= int, default= 1209)\n",
    "parser.add_argument('-num_micr', type= int, default= 172)\n",
    "parser.add_argument('-num_dise', type= int, default= 154)\n",
    "# drug_micr_rel, 0; drug_dise_rel, 1; micr_dise_rel, 2; drug_inter_rel, 3; micr_inter_rel, 4; micr_drug_rel, 5; dise_drug_rel, 6; dies_micr_rel, 7.\n",
    "parser.add_argument('-drug_name_path', type= str, default= '../mdd/drug/drug_name.txt')\n",
    "parser.add_argument('-micr_name_path', type= str, default= '../mdd/microbe/microbe_name.txt')\n",
    "parser.add_argument('-dise_name_path', type= str, default= '../mdd/disease/disease_name.txt')\n",
    "parser.add_argument('-drug_micr_adj_path', type= str, default= '../mdd/adj/microbe_drug_adj.txt')\n",
    "parser.add_argument('-drug_struct_simi_path', type= str, default= '../mdd/drug/drug_struct_simi.txt')\n",
    "parser.add_argument('-drug_inter_adj_path', type= str, default= '../mdd/drug/drug_interact_adj.txt')\n",
    "parser.add_argument('-drug_dise_adj_path', type= str, default= '../mdd/adj/drug_disease_adj.txt')\n",
    "parser.add_argument('-micr_ani_path', type= str, default= '../mdd/microbe/microbe_ani_simi.txt')\n",
    "parser.add_argument('-micr_fun_path', type= str, default= '../mdd/microbe/microbe_gene_simi.txt')\n",
    "parser.add_argument('-micr_inter_adj_path', type= str, default= '../mdd/microbe/microbe_interact_adj.txt')\n",
    "parser.add_argument('-micr_dise_adj_path', type= str, default= '../mdd/adj/microbe_disease_adj.txt')\n",
    "parser.add_argument('-dise_simi_path', type= str, default= '../mdd/disease/disease_dag_simi.txt')\n",
    "parser.add_argument('-train_ratio', type= float, default= 0.8)\n",
    "parser.add_argument('-valid_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-test_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-batch_size', type= int, default= 128)\n",
    "parser.add_argument('-hid_dim', type= int, default= 128)\n",
    "parser.add_argument('-out_dim', type= int, default= 3)\n",
    "parser.add_argument('-heads', type= int, default= 128)\n",
    "parser.add_argument('-dropout', type= int, default= 64)\n",
    "parser.add_argument('-alpha', type= int, default= 32)\n",
    "parser.add_argument('-threshold', type= float, default= 0)\n",
    "parser.add_argument('-epochs', type= int, default= 300)\n",
    "parser.add_argument('-patience', type= int, default= 6)\n",
    "parser.add_argument('-lr', type= float, default= 1e-4)\n",
    "parser.add_argument('-weight_decay', type= float, default= 0)\n",
    "parser.add_argument('-device', type= str, default= 'cuda:0')\n",
    "parser.add_argument('-pt_file', type= str, default= 'checkpoint/')\n",
    "parser.add_argument('-memo_file4mkgcn', type= str, default= 'memo/gsamda.txt')\n",
    "parser.add_argument('-pt_file_name', type= str, default= 'gsamda.pt')\n",
    "parser.add_argument('-test_result_file', type= str, default= 'result/gsamda_test_result.txt')\n",
    "params= parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754d133-e627-465b-9ce7-fe6f9d6abd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed_all(params.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(params.seed)    \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ce5de-8eff-4511-ac68-3afde274b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(object):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params= params\n",
    "        self.drug_micr_asso_mat= self.load_adj_data(self.params.drug_micr_adj_path, sp= (self.params.num_drug, self.params.num_micr))\n",
    "        self.train_xy, self.valid_xy, self.test_xy= self.split_dataset()\n",
    "        self.drug_micr_asso_mat, self.drug_micr_asso_mat_zy, self.drug_dise_asso_mat, self.drug_inter_mat, self.drug_struct_simi_mat= self.load_drug_data()\n",
    "        self.micr_ani_mat, self.micr_inter_mat, self.micr_dise_asso_mat, self.micr_asso_simi_mat, self.micr_fun_simi_mat= self.load_micr_data()\n",
    "        self.dise_simi_mat, self.drug_dise_drug_simi_mat, self.micr_dise_micr_simi_mat= self.load_dise_data()\n",
    "        self.drug_dise_simi_mat, self.micr_dise_simi_mat= self.cos_sim(self.drug_dise_asso_mat).float(), self.cos_sim(self.micr_dise_asso_mat).float()\n",
    "        self.drug_hip_simi_mat, self.micr_hip_simi_mat= self.hip_sim(self.drug_micr_asso_mat_zy).float(), self.hip_sim(self.drug_micr_asso_mat_zy.T).float()\n",
    "        self.drug_gip_simi_mat, self.micr_gip_simi_mat= self.gauss_sim(self.drug_micr_asso_mat_zy).float(), self.gauss_sim(self.drug_micr_asso_mat_zy.T).float()\n",
    "        self.drug_int_simi_mat, self.micr_int_simi_mat= (self.drug_hip_simi_mat+ self.drug_gip_simi_mat).float()/ 2, (self.micr_hip_simi_mat+ self.micr_gip_simi_mat).float()/2\n",
    "        self.drug_rwr_mat, self.micr_rwr_mat= self.rwr(self.drug_int_simi_mat).float(), self.rwr(self.micr_int_simi_mat).float()\n",
    "        self.hete_graph_mat= torch.cat([torch.cat([self.drug_int_simi_mat, self.drug_micr_asso_mat_zy], dim= 1),\\\n",
    "                                    torch.cat([self.drug_micr_asso_mat_zy.T, self.micr_int_simi_mat], dim= 1)], dim= 0).float()\n",
    "        # self.hete_graph_idx= torch.nonzero(self.hete_graph_mat> self.params.threshold).to(torch.long)\n",
    "    # @ 计算余弦相似性\n",
    "    def cos_sim(self, mat):\n",
    "        return torch.matmul(nn.functional.normalize(mat, p= 2, dim= 1), nn.functional.normalize(mat, p= 2, dim= 1).T)\n",
    "\n",
    "    # @ 计算高斯核相似性\n",
    "    def gauss_sim(self, mat):\n",
    "        # 高斯核\n",
    "        sigma= 1/ torch.diag(torch.matmul(mat, mat.T)).mean()\n",
    "        # 向量写法\n",
    "        sim_mat= torch.mul(mat, mat).sum(dim= 1, keepdims= True)+ torch.mul(mat, mat).sum(dim= 1, keepdims= True).T- 2* torch.matmul(mat, mat.T)\n",
    "        # 返回高斯核相似性矩阵\n",
    "        sim_mat= torch.exp(-1* sigma* sim_mat)\n",
    "        # MTK...\n",
    "        # sim_mat= 1/ (1+ torch.exp(-15* sim_mat+ math.log(9999)))\n",
    "        return sim_mat\n",
    "        \n",
    "    # @ 带重启的随机游走, mat, like, (1373, 1373)\n",
    "    def rwr(self, mat, times= 100):\n",
    "        # 0.1概率继续走, 0.9的概率回初始状态\n",
    "        alpha= 0.1\n",
    "        # 行归一化\n",
    "        trans_mat= mat/ (mat.sum(dim= 1, keepdims= True)+ 1e-15)\n",
    "        state_mat= torch.eye(mat.shape[0])\n",
    "        # 游走\n",
    "        for i in range(times):\n",
    "            state_mat= alpha* torch.matmul(trans_mat, state_mat)+ (1- alpha)* torch.eye(mat.shape[0])\n",
    "        return state_mat\n",
    "    \n",
    "    # @ 计算hamming interaction profile similarity.\n",
    "    def hip_sim(self, mat):\n",
    "        sim_ls, dim= [], mat.shape[1]\n",
    "        for i in range(mat.shape[0]):\n",
    "            sim_ls.append(((mat[i]- mat) == 0).sum(dim= 1)/ dim)\n",
    "        return torch.stack(sim_ls)\n",
    "\n",
    "    \n",
    "    # @introduce data\n",
    "    def introduce(self):\n",
    "        print(f'Drug microbe association num: {self.drug_micr_asso_mat.sum()}\\nDrug interaction num: {self.drug_inter_mat.sum()}\\nMicrobe interaction num: {self.micr_inter_mat.sum()}')\n",
    "        print(f'Drug disease association num: {self.drug_dise_asso_mat.sum()}\\nMicrobe disease association num: {self.micr_dise_asso_mat.sum()}')\n",
    "\n",
    "    # @ mask\n",
    "    def get_asso_mat_zy(self):\n",
    "        asso_mat_zy= self.drug_micr_asso_mat.clone()\n",
    "        asso_mat_zy[self.valid_xy[:, 0], self.valid_xy[:, 1]]= 0\n",
    "        asso_mat_zy[self.test_xy[:, 0], self.test_xy[:, 1]]= 0\n",
    "        return asso_mat_zy\n",
    "\n",
    "    # @split data set\n",
    "    def split_dataset(self):\n",
    "        train_xy, valid_xy, test_xy= [], [], []\n",
    "        for i in range(self.params.num_drug):\n",
    "            first= True\n",
    "            for j in range(self.params.num_micr):\n",
    "                if self.drug_micr_asso_mat[i, j]== 1 and first:\n",
    "                    train_xy.append([i, j])\n",
    "                    first= False\n",
    "                else:\n",
    "                    num= torch.rand(1)\n",
    "                    if num< self.params.train_ratio:\n",
    "                        train_xy.append([i, j])\n",
    "                    elif num>= self.params.train_ratio and num< self.params.train_ratio+ self.params.valid_ratio:\n",
    "                        valid_xy.append([i, j])\n",
    "                    else:\n",
    "                        test_xy.append([i, j])        \n",
    "        print(f'Spliting data has finished...')\n",
    "        return torch.tensor(train_xy), torch.tensor(valid_xy), torch.tensor(test_xy)\n",
    "\n",
    "    # @load disease data\n",
    "    def load_dise_data(self):\n",
    "        dise_simi_mat= torch.from_numpy(np.loadtxt(self.params.dise_simi_path, encoding= 'utf-8-sig'))\n",
    "        drug_dise_drug_simi_mat, micr_dise_micr_simi_mat= torch.matmul(nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1).T),\\\n",
    "        torch.matmul(nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1).T)\n",
    "        for i in range(self.params.num_drug): drug_dise_drug_simi_mat[i, i]= 1.0\n",
    "        for i in range(self.params.num_micr): micr_dise_micr_simi_mat[i, i]= 1.0\n",
    "        return dise_simi_mat, drug_dise_drug_simi_mat, micr_dise_micr_simi_mat\n",
    "\n",
    "    # @load micr data\n",
    "    def load_micr_data(self):\n",
    "        micr_ani_mat= torch.from_numpy(np.loadtxt(self.params.micr_ani_path, encoding= 'utf-8-sig'))\n",
    "        micr_inter_mat= self.load_adj_data(self.params.micr_inter_adj_path, sp= (self.params.num_micr, self.params.num_micr))\n",
    "        micr_dise_mat= self.load_adj_data(self.params.micr_dise_adj_path, sp= (self.params.num_micr, self.params.num_dise))\n",
    "        micr_asso_simi_mat= torch.matmul(nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1), nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1).T)\n",
    "        micr_fun_simi_mat= torch.from_numpy(np.loadtxt(self.params.micr_fun_path, encoding= 'utf-8-sig'))\n",
    "        for i in range(self.params.num_micr):micr_asso_simi_mat[i, i]= 1\n",
    "        for i in range(self.params.num_micr):micr_ani_mat[i, i]= 1            \n",
    "        return micr_ani_mat.float(), micr_inter_mat.float(), micr_dise_mat.float(), micr_asso_simi_mat.float(), micr_fun_simi_mat.float()\n",
    "\n",
    "    # @load drug data\n",
    "    def load_drug_data(self):\n",
    "        drug_dise_asso_mat= self.load_adj_data(self.params.drug_dise_adj_path, sp= (self.params.num_drug, self.params.num_dise))\n",
    "        drug_micr_asso_mat= self.load_adj_data(self.params.drug_micr_adj_path, sp= (self.params.num_drug, self.params.num_micr))\n",
    "        drug_micr_asso_mat_zy= drug_micr_asso_mat.clone()\n",
    "        drug_micr_asso_mat_zy[self.valid_xy[:, 0], self.valid_xy[:, 1]]= 0\n",
    "        drug_micr_asso_mat_zy[self.test_xy[:, 0], self.test_xy[:, 1]]= 0\n",
    "        drug_inter_mat= self.load_adj_data(self.params.drug_inter_adj_path, sp= (self.params.num_drug, self.params.num_drug))\n",
    "        drug_struct_simi_mat= torch.from_numpy(np.loadtxt(self.params.drug_struct_simi_path, encoding= 'utf-8-sig'))\n",
    "        return drug_micr_asso_mat.float(), drug_micr_asso_mat_zy.float(), drug_dise_asso_mat.float(), drug_inter_mat.float(), drug_struct_simi_mat.float()\n",
    "    \n",
    "    # @load adj data\n",
    "    def load_adj_data(self, path, sp= (1209, 172)):\n",
    "        idx= torch.from_numpy(np.loadtxt(path, encoding= 'utf-8-sig')).long()- 1\n",
    "        mat= torch.zeros((sp[0], sp[1]))\n",
    "        mat[idx[:, 0], idx[:, 1]]= 1\n",
    "        return mat\n",
    "\n",
    "    # write into memo\n",
    "    def write2memo(self, mr, mrr, hits10):\n",
    "        with open(f'{self.params.memo_file4kg}', 'a+') as f:\n",
    "            f.write(f'{self.params.lr_kg}\\t{self.params.weight_decay_kg}\\t{mr}\\t{mrr}\\t{hits10}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4ca75-5ee2-447a-99fd-241937f8d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl= dataloader(params)\n",
    "train_label, valid_label, test_label= dl.drug_micr_asso_mat[dl.train_xy[:, 0], dl.train_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.valid_xy[:, 0], dl.valid_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.test_xy[:, 0], dl.test_xy[:, 1]].long()\n",
    "# train\n",
    "train_xy_label_dataset= torch.utils.data.TensorDataset(dl.train_xy, train_label)\n",
    "train_loader= torch.utils.data.DataLoader(train_xy_label_dataset, batch_size= params.batch_size, shuffle= True)\n",
    "# valid\n",
    "valid_xy_label_dataset= torch.utils.data.TensorDataset(dl.valid_xy, valid_label)\n",
    "valid_loader= torch.utils.data.DataLoader(valid_xy_label_dataset, batch_size= params.batch_size, shuffle= False)\n",
    "# test, \n",
    "test_xy_label_dataset= torch.utils.data.TensorDataset(dl.test_xy, test_label)\n",
    "test_loader= torch.utils.data.DataLoader(test_xy_label_dataset, batch_size= params.batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33198a71-9a03-4cb7-b35b-ea85f5ef87ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\t\"\"\"docstring for EarlyStopping\"\"\"\n",
    "\tdef __init__(self, patience, pt_file= 'checkpoint/', file_name= 'checkpoint.pt', mess_out= True, eps= 0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.patience, self.eps, self.pt_file, self.file_name, self.mess_out= patience, eps, pt_file, file_name, mess_out\n",
    "\t\tself.best_score, self.counter, self.flag= None, 0, False\n",
    "\t\tif os.path.exists(self.pt_file)== False:os.makedirs(self.pt_file)\n",
    "\t\n",
    "\tdef __call__(self, val_loss, model):\n",
    "\t\tscore= -val_loss\n",
    "\t\tif self.best_score is None:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\telif score< self.best_score- self.eps:\n",
    "\t\t\tself.counter+= 1\n",
    "\t\t\tif self.mess_out:print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "\t\t\tif self.counter>= self.patience:\n",
    "\t\t\t\tself.flag= True\n",
    "\t\telse:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\t\tself.counter= 0\n",
    "\n",
    "\tdef save_checkpoint(self, model):\n",
    "\t\ttorch.save(model, f'{self.pt_file}//{self.file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e745ff4-fdd5-4fed-8809-fd9840f9d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ Gat层\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "\tdef __init__(self, in_dim, out_dim, dropout, alpha, concat= True):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# \n",
    "\t\tself.out_dim= out_dim\n",
    "\t\tself.leakyrelu= nn.LeakyReLU(alpha)\n",
    "\t\tself.dropout= dropout\n",
    "\t\tself.W= nn.Parameter(torch.zeros(size= (in_dim, out_dim)))\n",
    "\t\tself.a= nn.Parameter(torch.zeros(size= (2* out_dim, 1)))\n",
    "\t\tnn.init.xavier_uniform_(self.W.data, nn.init.calculate_gain('leaky_relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.a.data, nn.init.calculate_gain('leaky_relu'))\n",
    "\tdef forward(self, x, adj_mat):\n",
    "\t\t# W, (in_dim, out_dim); x, (1546, in_dim); x, (1546, out_dim);\n",
    "\t\tx= torch.matmul(x, self.W)\n",
    "\t\t# 计算注意力\n",
    "\t\t# x, (1546, out_dim);(out_dim, 1); el, (1546, 1);\n",
    "\t\tel= torch.matmul(x, self.a[0: self.out_dim])\n",
    "\t\ter= torch.matmul(x, self.a[self.out_dim:]).T\n",
    "\t\tatt_hat= self.leakyrelu(el+ er)\n",
    "\t\tzero_mat= -9e15* torch.ones_like(att_hat)\n",
    "\t\tatt_hat= torch.where(adj_mat> 0, att_hat, zero_mat)\n",
    "\t\tatt= F.softmax(att_hat, dim= 1)\n",
    "\t\tatt= F.dropout(att, self.dropout, training= self.training)\n",
    "\t\t# 聚合消息, (1546, out_dim)\n",
    "\t\tx= torch.matmul(att, x)\n",
    "\t\treturn F.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27da43c-98af-4966-9140-50761c426179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ CNN层\n",
    "class CNN(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(CNN, self).__init__()\n",
    "\t\tself.seq1=nn.Sequential(nn.Conv2d(1,16,kernel_size=3,padding=1),nn.BatchNorm2d(16),nn.ReLU())\n",
    "\t\tself.seq2=nn.Sequential(nn.Conv2d(16,6,kernel_size=3,padding=1),nn.BatchNorm2d(6),nn.ReLU())\n",
    "\t\tself.fc= nn.Sequential(nn.Linear(38640, 256), nn.Dropout(0.5), nn.ReLU(), nn.Linear(256, 2))\n",
    "\t\tnn.init.xavier_uniform_(self.fc[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.fc[3].weight)\n",
    "\n",
    "    # x, (batch_size, 1, 2, -1)\n",
    "\tdef forward(self, x):\n",
    "\t\tx=self.seq1(x)\n",
    "\t\tx=self.seq2(x)\n",
    "\t\treturn self.fc(x.view(x.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af833391-4e4e-41a3-8bf5-7adc19e92529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ gat\n",
    "class GAT(nn.Module):\n",
    "\tdef __init__(self, in_dim, hid_dim, out_dim, heads, dropout, alpha= 0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# gat层1\n",
    "\t\tself.attention_layer1= nn.ModuleList([GraphAttentionLayer(in_dim, hid_dim// heads, dropout, alpha) for i in range(heads)])\n",
    "\t\t# gat层2\n",
    "\t\tself.attention_layer2= GraphAttentionLayer(hid_dim, out_dim, dropout, alpha)\n",
    "\tdef forward(self, x, adj_mat):\n",
    "\t\t# 考虑多头\n",
    "\t\temb= []\n",
    "\t\tfor attention_layer in self.attention_layer1:\n",
    "\t\t\temb.append(attention_layer(x, adj_mat))\n",
    "\t\temb= torch.cat(emb, dim= 1)\n",
    "\t\t# (1546, 128)\n",
    "\t\tx= self.attention_layer2(emb, adj_mat)\n",
    "\t\treturn x, torch.sigmoid(torch.matmul(x, x.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bd83b-bb51-4b80-8403-8dc0b5babe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动编码器\n",
    "class AE(nn.Module):\n",
    "\tdef __init__(self, in_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder= nn.Sequential(nn.Linear(in_dim, 128), nn.ReLU(), nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU())\n",
    "\t\tself.decoder= nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, in_dim), nn.Sigmoid())\n",
    "\t\tnn.init.xavier_uniform_(self.encoder[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.encoder[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.encoder[4].weight, nn.init.calculate_gain('relu'))\t\t\n",
    "\t\tnn.init.xavier_uniform_(self.decoder[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.decoder[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.decoder[4].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\tdef forward(self, x):\n",
    "\t\temb= self.encoder(x)\n",
    "\t\tx_hat= self.decoder(emb)\n",
    "\t\treturn emb, x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eca758-aadb-47f9-bd84-dcad7383202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\tdef __init__(self, in_dim= 4798):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.fc= nn.Sequential(nn.Linear(in_dim* 2, 1024), nn.ReLU(), nn.Dropout(0.2), nn.Linear(1024, 64), nn.ReLU(), nn.Dropout(0.2), nn.Linear(64, 1), nn.Sigmoid())\n",
    "\t\tnn.init.xavier_uniform_(self.fc[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.fc[3].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_uniform_(self.fc[6].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b243eec-3162-4415-8726-35e693491077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSAMDA(nn.Module):\n",
    "\tdef __init__(self, fea4drug, fea4micro, x, graph1, in_dim, hid_dim, out_dim, heads, dropout, alpha, loss_mse, loss_kl, ass_mat1, drug_struct_sim, micro_funct_sim, drug_feaByrwr1, micro_feaByrwr1, Sr_dis, Sm_dis):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.fea4drug, self.fea4micro, self.g1, self.mse, self.loss_kl, self.x= fea4drug, fea4micro, graph1, loss_mse, loss_kl, x\n",
    "\t\tself.gat4graph= GAT(in_dim, hid_dim, out_dim, heads, dropout, alpha)\n",
    "\t\tself.ae4drug, self.ae4micro= AE(fea4drug.shape[1]), AE(fea4micro.shape[1])\n",
    "\t\tself.fc= MLP()\n",
    "\t\tself.ass_mat1, self.drug_struct_sim, self.micro_funct_sim, self.drug_feaByrwr1, self.micro_feaByrwr1, self.Sr_dis, self.Sm_dis= ass_mat1, drug_struct_sim, micro_funct_sim, drug_feaByrwr1, micro_feaByrwr1, Sr_dis, Sm_dis\n",
    "\n",
    "\tdef forward(self, left, right):\n",
    "\t\t# 先过gat\n",
    "\t\temb1, recon1= self.gat4graph(self.x, self.g1)\n",
    "\t\tgat_emb4drug, gat_emb4micro= emb1[0: 1209], emb1[1209:]\n",
    "\t\t# ae4drug\n",
    "\t\t(emb4drug, recon4drug), (emb4micro, recon4micro)= self.ae4drug(self.fea4drug), self.ae4micro(self.fea4micro)\n",
    "\t\t# ls_loss\n",
    "\t\temb4drug_exp_act_val, emb4micro_exp_act_val= 0.05* torch.ones_like(emb4drug), 0.05* torch.ones_like(emb4micro)\n",
    "\t\tkl_loss4drug= self.loss_kl(F.log_softmax(emb4drug, dim= 1), F.softmax(emb4drug_exp_act_val, dim= 1))\n",
    "\t\tkl_loss4micro= self.loss_kl(F.log_softmax(emb4micro, dim= 1), F.softmax(emb4micro_exp_act_val, dim= 1))\n",
    "\t\tkl_loss= kl_loss4drug+ kl_loss4micro\n",
    "\t\t# 构建特征\n",
    "\t\tdrug_fea= torch.cat((gat_emb4drug, emb4drug, self.drug_struct_sim, self.ass_mat1, self.Sr_dis, self.ass_mat1, self.drug_feaByrwr1, self.ass_mat1), dim= 1)\n",
    "\t\tmicro_fea= torch.cat((gat_emb4micro, emb4micro, self.ass_mat1.T, self.micro_funct_sim, self.ass_mat1.T, self.Sm_dis, self.ass_mat1.T, self.micro_feaByrwr1), dim= 1)\n",
    "\t\t# drug_fea, (1373, 4456); micro_fea, (173, 4456)\n",
    "\t\tass_mat_hat= torch.matmul(F.normalize(drug_fea, p= 2, dim= 1), F.normalize(micro_fea, p= 2, dim= 1).T)\n",
    "\t\tfea= torch.cat((drug_fea, micro_fea), dim= 0).to(torch.float)\n",
    "\t\t# 重构损失\n",
    "\t\trecon_loss= self.mse(recon1, self.g1)+ self.mse(recon4drug, self.fea4drug)+ self.mse(recon4micro, self.fea4micro)+ 0.1* kl_loss\n",
    "\t\treturn ass_mat_hat[left, right].to(torch.float32), recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4218912-f751-4ae8-a909-50a05462e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_auc_aupr_cpt(test_xy, test_label, pred, ass_mat_shape):\n",
    "    label_mat, pred_mat= torch.zeros((ass_mat_shape)) -1, torch.zeros((ass_mat_shape)) -1\n",
    "    label_mat[test_xy[:, 0], test_xy[:, 1]], pred_mat[test_xy[:, 0], test_xy[:, 1]]= test_label* 1.0, pred\n",
    "    bool_mat4mark_test_examp= (label_mat!= -1)\n",
    "    aucs, auprs= [], []\n",
    "    for i in range(ass_mat_shape[0]):\n",
    "        test_examp_loc= bool_mat4mark_test_examp[i]\n",
    "        pos_num= label_mat[i, test_examp_loc].sum()\n",
    "        if pos_num> 0 and (test_examp_loc).sum()- pos_num> 0:\n",
    "            fpr4rowi, tpr4rowi, _= roc_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi, recall4rowi, _= precision_recall_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi[-1]= [1, 0][(int)(prec4rowi[-2]== 0)]\n",
    "            aucs.append(auc(fpr4rowi, tpr4rowi));auprs.append(auc(recall4rowi, prec4rowi))\n",
    "    return np.mean(aucs), np.mean(auprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3155a-ba7f-4df9-b953-42c751865cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params.device= 'cpu'\n",
    "drug_fea= torch.cat([dl.drug_micr_asso_mat_zy, dl.drug_struct_simi_mat, dl.drug_micr_asso_mat_zy, dl.drug_rwr_mat, dl.drug_dise_simi_mat], dim= 1)\n",
    "micr_fea= torch.cat([dl.drug_micr_asso_mat_zy.T, dl.micr_ani_mat, dl.drug_micr_asso_mat_zy.T, dl.micr_rwr_mat, dl.micr_dise_simi_mat], dim= 1)\n",
    "loss_fuc, loss_mse, loss_kl= nn.BCELoss(), nn.MSELoss(reduction= 'mean'), nn.KLDivLoss(reduction= 'batchmean')\n",
    "net= GSAMDA(drug_fea.to(params.device), micr_fea.to(params.device), dl.hete_graph_mat.clone().to(params.device), dl.hete_graph_mat.to(params.device), dl.hete_graph_mat.shape[1], 256, 128, 1, 0.4, 0.2, loss_mse, loss_kl, dl.drug_micr_asso_mat_zy.to(params.device), dl.drug_struct_simi_mat.to(params.device), dl.micr_ani_mat.to(params.device), dl.drug_dise_simi_mat.to(params.device), dl.micr_dise_simi_mat.to(params.device), dl.drug_int_simi_mat.to(params.device), dl.micr_int_simi_mat.to(params.device)).to(params.device)\n",
    "optimizer= torch.optim.Adam(net.parameters(), lr= params.lr)\n",
    "earlystopping= EarlyStopping(patience= params.patience, pt_file= params.pt_file, file_name= params.pt_file_name, mess_out= True)\n",
    "pred= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0777b6b1-e4bd-4622-ab40-6eb015e14cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(params.epochs):\n",
    "    # train\n",
    "    for step, (xy, label) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        time_start= time.time()\n",
    "        xy, label= xy.to(params.device), label.to(params.device)\n",
    "        logp, loss2= net(xy[:, 0], xy[:, 1])\n",
    "        loss= loss_fuc(logp, label.float())+ loss2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_end= time.time()\n",
    "        if step% 500== 0:\n",
    "            print(f'epoch: {ep+ 1}, step: {step+ 1}, train loss: {loss}, time: {time_end- time_start}')\n",
    "    # valid\n",
    "    net.eval(); val_loss= 0; pred= []\n",
    "    with torch.no_grad():\n",
    "        for step, (xy, label) in enumerate(valid_loader):\n",
    "            xy, label= xy.to(params.device), label.to(params.device)\n",
    "            logp, loss2= net(xy[:, 0], xy[:, 1])\n",
    "            val_loss= loss_fuc(logp, label.float())+ loss2\n",
    "            pred.append(logp)\n",
    "    pred= torch.cat(pred).cpu()\n",
    "    roc_auc, aupr_auc= avg_auc_aupr_cpt(dl.valid_xy, valid_label, pred, (1209, 172))\n",
    "    print(f'epoch: {ep+ 1}, valid loss: {val_loss}, auc: {roc_auc}, aupr: {aupr_auc}')\n",
    "    earlystopping(-(roc_auc+ aupr_auc), net)\n",
    "    if earlystopping.flag== True:print(f'early_stopping');break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8b019-1837-4e2d-b0ce-fc69b091197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net= torch.load(f'{params.pt_file}//{params.pt_file_name}')\n",
    "net.eval();results= []\n",
    "with torch.no_grad():\n",
    "    for _, (xy, _) in enumerate(test_loader):\n",
    "        logp, _= net(xy[:, 0], xy[:, 1])\n",
    "        results.append(logp)\n",
    "pred= torch.cat(results, dim= 0)\n",
    "results= torch.cat([dl.test_xy.to('cpu'), test_label.view(-1, 1).to('cpu'), pred.view(-1, 1).to('cpu')], dim= 1)\n",
    "print(avg_auc_aupr_cpt(dl.test_xy.to('cpu'), test_label.to('cpu'), pred.to('cpu'), (1209, 172)))\n",
    "# np.savetxt(fname= params.test_result_file, X= results, delimiter= '\\t', encoding= 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b18c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
