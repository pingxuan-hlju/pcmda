{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import dgl\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "from dgl.utils import expand_as_pair\n",
    "from dgl.nn.pytorch import edge_softmax\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= argparse.ArgumentParser(description= 'EHGN4MDA')\n",
    "parser.add_argument('--root-path', type= str, default= os.path.abspath('..'))\n",
    "parser.add_argument('--seed', type= int, default= 0)\n",
    "parser.add_argument('--device', type= str, default= 'cuda:0')\n",
    "parser.add_argument('--hd4gat', type= int, default= 128)\n",
    "parser.add_argument('--hd4tf', type= int, default= 64)\n",
    "parser.add_argument('--layers_num', type= int, default= 2)\n",
    "parser.add_argument('--head4gat', type= int, default= 4)\n",
    "parser.add_argument('--head4transformer', type= int, default= 8)\n",
    "parser.add_argument('--feat_dropout', type= float, default= 0.5)\n",
    "parser.add_argument('--attn_dropout', type= float, default= 0.5)\n",
    "parser.add_argument('--decoder', type= str, default= 'mlp')\n",
    "parser.add_argument('--lr', type= float, default= 1e-4)\n",
    "parser.add_argument('--weight_decay', type= float, default= 5e-4)\n",
    "parser.add_argument('--patience', type= int, default= 6)\n",
    "parser.add_argument('--epochs', type= int, default= 300)\n",
    "parser.add_argument('--batch_size', type= int, default= 128)\n",
    "parser.add_argument('--kflod_num', type= int, default= 5)\n",
    "parser.add_argument('--lbda', type= int, default= 4)\n",
    "parser.add_argument('--wr_time', type= int, default= 2)\n",
    "parser.add_argument('--i', type= int, default= 0)\n",
    "parser.add_argument('--threshold', type= float, default= 0.8)\n",
    "parser.add_argument('-num_ent', type= int, default= 1209+ 172+ 154)\n",
    "parser.add_argument('-num_drug', type= int, default= 1209)\n",
    "parser.add_argument('-num_micr', type= int, default= 172)\n",
    "parser.add_argument('-num_dise', type= int, default= 154)\n",
    "# drug_micr_rel, 0; drug_dise_rel, 1; micr_dise_rel, 2; drug_inter_rel, 3; micr_inter_rel, 4; micr_drug_rel, 5; dise_drug_rel, 6; dies_micr_rel, 7.\n",
    "parser.add_argument('-drug_name_path', type= str, default= '../mdd/drug/drug_name.txt')\n",
    "parser.add_argument('-micr_name_path', type= str, default= '../mdd/microbe/microbe_name.txt')\n",
    "parser.add_argument('-dise_name_path', type= str, default= '../mdd/disease/disease_name.txt')\n",
    "parser.add_argument('-drug_micr_adj_path', type= str, default= '../mdd/adj/microbe_drug_adj.txt')\n",
    "parser.add_argument('-drug_struct_simi_path', type= str, default= '../mdd/drug/drug_struct_simi.txt')\n",
    "parser.add_argument('-drug_inter_adj_path', type= str, default= '../mdd/drug/drug_interact_adj.txt')\n",
    "parser.add_argument('-drug_fringer_simi_path', type= str, default= '../mdd/drug/drug_fringer_simi.txt')\n",
    "parser.add_argument('-drug_dise_adj_path', type= str, default= '../mdd/adj/drug_disease_adj.txt')\n",
    "parser.add_argument('-micr_ani_path', type= str, default= '../mdd/microbe/microbe_ani_simi.txt')\n",
    "parser.add_argument('-micr_inter_adj_path', type= str, default= '../mdd/microbe/microbe_interact_adj.txt')\n",
    "parser.add_argument('-micr_dise_adj_path', type= str, default= '../mdd/adj/microbe_disease_adj.txt')\n",
    "parser.add_argument('-micr_gene_simi_path', type= str, default= '../mdd/microbe/microbe_gene_simi.txt')\n",
    "parser.add_argument('-dise_simi_path', type= str, default= '../mdd/disease/disease_dag_simi.txt')\n",
    "parser.add_argument('-train_ratio', type= float, default= 0.8)\n",
    "parser.add_argument('-valid_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-test_ratio', type= float, default= 0.1)\n",
    "parser.add_argument('-pt_file', type= str, default= 'checkpoint/')\n",
    "parser.add_argument('-memo_file4ngmda', type= str, default= 'memo/ngmda.txt')\n",
    "parser.add_argument('-pt_file_name', type= str, default= 'ngmda.pt')\n",
    "parser.add_argument('-test_result_file', type= str, default= 'result/ngmda_test_result.txt')\n",
    "params= parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(params.seed)\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed_all(params.seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(params.seed)    \n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataloader(object):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.params= params\n",
    "        self.drug_micr_asso_mat, self.drug_struct_simi_mat= self.load_drug_data()\n",
    "        self.train_xy, self.valid_xy, self.test_xy= self.split_dataset()\n",
    "        self.drug_micr_asso_mat_zy= self.get_asso_mat_zy()\n",
    "        self.micr_ani_mat, self.micr_gene_simi_mat, self.micr_inte_simi_mat= self.load_micr_data()\n",
    "        self.hete_graph01_mat= torch.cat([torch.cat([self.drug_struct_simi_mat>= self.params.threshold, self.drug_micr_asso_mat_zy], dim= 1),\\\n",
    "                                    torch.cat([self.drug_micr_asso_mat_zy.T, self.micr_inte_simi_mat>= self.params.threshold], dim= 1)], dim= 0).float()\n",
    "        self.fea_mats= [self.drug_struct_simi_mat, self.micr_inte_simi_mat, self.drug_struct_simi_mat>= self.params.threshold, self.micr_inte_simi_mat>= self.params.threshold, self.drug_micr_asso_mat_zy]\n",
    "        self.topo_fea= self.acquire_topo_fea(self.hete_graph01_mat, self.params.wr_time)\n",
    "        self.g, self.e_feat= self.get_hete_graph(self.hete_graph01_mat)\n",
    "\n",
    "    def get_hete_graph(self, adj_mat):\n",
    "        x_y= adj_mat.nonzero()\n",
    "        x, y, y_offset= x_y[:, 0], x_y[:, 1], torch.tensor(1209)\n",
    "        adj= sp.coo_matrix((torch.ones(len(x)), (x, y)), shape= (1381, 1381)).tocsr()        \n",
    "        g= dgl.DGLGraph(adj)\n",
    "        g= dgl.add_self_loop(g)\n",
    "        e_feat= []\n",
    "        for u, v in zip(*g.edges()):\n",
    "            u, v= u.item(), v.item()\n",
    "            if u< 1209 and v< 1209:\n",
    "                e_type= 0\n",
    "            elif u< 1209 and v>= 1209:\n",
    "                e_type= 1\n",
    "            elif u>= 1209 and v< 1209:\n",
    "                e_type= 2\n",
    "            else: \n",
    "                e_type= 3\n",
    "            e_feat.append(e_type)\n",
    "        # g中每条边的边类型信息\n",
    "        e_feat= torch.tensor(e_feat, dtype= torch.long)\n",
    "        return g, e_feat\n",
    "        \n",
    "    def acquire_topo_fea(self, ass_mat, k):\n",
    "    \ttopo_fea= torch.zeros((1381, k))\n",
    "    \td_mat= torch.diag(ass_mat.sum(dim= 1)).inverse()\n",
    "    \ttopo_mat= torch.matmul(ass_mat, d_mat)\n",
    "    \tfor i in range(k):\n",
    "    \t\ttopo_fea[:, i]= torch.diag(topo_mat)\n",
    "    \t\ttopo_mat= torch.matmul(topo_mat, topo_mat)\n",
    "    \treturn topo_fea\n",
    "        \n",
    "    # @introduce data\n",
    "    def introduce(self):\n",
    "        print(f'Drug microbe association num: {self.drug_micr_asso_mat.sum()}\\nDrug interaction num: {self.drug_inter_mat.sum()}\\nMicrobe interaction num: {self.micr_inter_mat.sum()}')\n",
    "        print(f'Drug disease association num: {self.drug_dise_asso_mat.sum()}\\nMicrobe disease association num: {self.micr_dise_asso_mat.sum()}')\n",
    "\n",
    "    # @ mask\n",
    "    def get_asso_mat_zy(self):\n",
    "        asso_mat_zy= self.drug_micr_asso_mat.clone()\n",
    "        asso_mat_zy[self.valid_xy[:, 0], self.valid_xy[:, 1]]= 0\n",
    "        asso_mat_zy[self.test_xy[:, 0], self.test_xy[:, 1]]= 0\n",
    "        return asso_mat_zy\n",
    "\n",
    "    # @split data set\n",
    "    def split_dataset(self):\n",
    "        train_xy, valid_xy, test_xy= [], [], []\n",
    "        for i in range(self.params.num_drug):\n",
    "            first= True\n",
    "            for j in range(self.params.num_micr):\n",
    "                if self.drug_micr_asso_mat[i, j]== 1 and first:\n",
    "                    train_xy.append([i, j])\n",
    "                    first= False\n",
    "                else:\n",
    "                    num= torch.rand(1)\n",
    "                    if num< self.params.train_ratio:\n",
    "                        train_xy.append([i, j])\n",
    "                    elif num>= self.params.train_ratio and num< self.params.train_ratio+ self.params.valid_ratio:\n",
    "                        valid_xy.append([i, j])\n",
    "                    else:\n",
    "                        test_xy.append([i, j])        \n",
    "        print(f'Spliting data has finished...')\n",
    "        return torch.tensor(train_xy), torch.tensor(valid_xy), torch.tensor(test_xy)\n",
    "\n",
    "    # @load disease data\n",
    "    def load_dise_data(self):\n",
    "        dise_simi_mat= torch.from_numpy(np.loadtxt(self.params.dise_simi_path, encoding= 'utf-8-sig'))\n",
    "        drug_dise_drug_simi_mat, micr_dise_micr_simi_mat= torch.matmul(nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.drug_inter_mat, p= 2, dim= 1).T),\\\n",
    "        torch.matmul(nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1), nn.functional.normalize(self.micr_inter_mat, p= 2, dim= 1).T)\n",
    "        for i in range(self.params.num_drug): drug_dise_drug_simi_mat[i, i]= 1.0\n",
    "        for i in range(self.params.num_micr): micr_dise_micr_simi_mat[i, i]= 1.0\n",
    "        return dise_simi_mat, drug_dise_drug_simi_mat, micr_dise_micr_simi_mat\n",
    "\n",
    "    # @load micr data\n",
    "    def load_micr_data(self):\n",
    "        micr_ani_mat= torch.from_numpy(np.loadtxt(self.params.micr_ani_path, encoding= 'utf-8-sig'))\n",
    "        micr_gene_simi_mat= torch.from_numpy(np.loadtxt(self.params.micr_gene_simi_path, encoding= 'utf-8-sig'))\n",
    "        micr_inter_mat= self.load_adj_data(self.params.micr_inter_adj_path, sp= (self.params.num_micr, self.params.num_micr))\n",
    "        micr_dise_mat= self.load_adj_data(self.params.micr_dise_adj_path, sp= (self.params.num_micr, self.params.num_dise))\n",
    "        micr_asso_simi_mat= torch.matmul(nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1), nn.functional.normalize(self.drug_micr_asso_mat_zy.T, p= 2, dim= 1).T)\n",
    "        for i in range(self.params.num_micr):micr_asso_simi_mat[i, i]= 1\n",
    "        for i in range(self.params.num_micr):micr_ani_mat[i, i]= 1            \n",
    "        return micr_ani_mat, micr_gene_simi_mat, torch.where(micr_ani_mat> 0, micr_ani_mat, micr_gene_simi_mat)\n",
    "\n",
    "    # @load drug data\n",
    "    def load_drug_data(self):\n",
    "        drug_micr_asso_mat= self.load_adj_data(self.params.drug_micr_adj_path, sp= (self.params.num_drug, self.params.num_micr))\n",
    "        drug_struct_simi_mat= torch.from_numpy(np.loadtxt(self.params.drug_struct_simi_path, encoding= 'utf-8-sig'))\n",
    "        return drug_micr_asso_mat, drug_struct_simi_mat\n",
    "    \n",
    "    # @load adj data\n",
    "    def load_adj_data(self, path, sp= (1209, 172)):\n",
    "        idx= torch.from_numpy(np.loadtxt(path, encoding= 'utf-8-sig')).long()- 1\n",
    "        mat= torch.zeros((sp[0], sp[1]))\n",
    "        mat[idx[:, 0], idx[:, 1]]= 1\n",
    "        return mat\n",
    "    \n",
    "    # @NRWR, neighborhood random walk with restart\n",
    "    def random_walk_root(self, A, alpha, epoch, nei, eps, err):\n",
    "        Mask_mat= (torch.matrix_power(A, nei)- torch.matrix_power(A, nei- 1))> 0\n",
    "        W= nn.functional.normalize(A, p= 1, dim= 1)\n",
    "        S_last= torch.eye(A.shape[0])\n",
    "        for i in range(epoch):\n",
    "            S_new= alpha* torch.matmul(S_last, W)+ (1- alpha)* torch.eye(A.shape[0])\n",
    "            S_new= nn.functional.normalize(Mask_mat* S_new, p= 1, dim= 1)\n",
    "            if (S_new- S_last).abs().sum()<= err* A.shape[0]** 2:\n",
    "                print('converge...');break\n",
    "            S_last= S_new\n",
    "        return S_new\n",
    "\n",
    "    # write into memo\n",
    "    def write2memo(self, mr, mrr, hits10):\n",
    "        with open(f'{self.params.memo_file4kg}', 'a+') as f:\n",
    "            f.write(f'{self.params.lr_kg}\\t{self.params.weight_decay_kg}\\t{mr}\\t{mrr}\\t{hits10}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNConv(nn.Module):\n",
    "\tdef __init__(self, in_feats, out_feats, num_heads, feat_drop= 0., attn_drop= 0.):\n",
    "\t\tsuper(HGNConv, self).__init__()\n",
    "\t\tself._num_heads, self._out_feats, self.node_type_num= num_heads, out_feats, 2\n",
    "\t\tself.fc4node_proj= nn.ModuleList([nn.Linear(in_feats, out_feats* num_heads, bias= False) for i in range(self.node_type_num)])\n",
    "\t\tself.fc4prob= nn.ModuleList([nn.Linear(out_feats, out_feats, bias= False) for i in range(self._num_heads)])\n",
    "\t\t# 边类型attn 因为一共四种类型的关系 d2d, d2m, m2d, m2m\n",
    "\t\tself.etype_attn= nn.Parameter(torch.tensor([1, 1, 1, 1]).to(torch.float))\n",
    "\t\tself.feat_drop4cont, self.feat_drop4loc, self.attn_drop= nn.Dropout(feat_drop), nn.Dropout(feat_drop), nn.Dropout(attn_drop)\n",
    "\t\t# fc 4 residual\n",
    "\t\tself.fc4res= nn.ModuleList([nn.Linear(in_feats, out_feats* num_heads, bias= False) for i in range(self.node_type_num)])\n",
    "\t\t# tao为1 即没有拓扑\n",
    "\t\tself.relu, self.tao= nn.ReLU(), 0.5\n",
    "\t\tself.reset_parameters()\n",
    "\n",
    "\t# 初始化权重\n",
    "\tdef reset_parameters(self):\n",
    "\t\tfor fc in self.fc4node_proj: nn.init.xavier_normal_(fc.weight)\n",
    "\t\tfor fc in self.fc4res: nn.init.xavier_normal_(fc.weight)\n",
    "\t\tfor fc in self.fc4prob: nn.init.xavier_normal_(fc.weight, gain= nn.init.calculate_gain('tanh'))\n",
    "\n",
    "\t# 计算注意力\n",
    "\tdef edge_attention(self, edges):\n",
    "\t\tsrc_feat_nrom, src_sloc, dst_sloc, src_tp_fea, dst_tp_fea= edges.src['ft_norm'], edges.src['sloc'], edges.dst['sloc'], edges.src['topo_fea'], edges.dst['topo_fea']\n",
    "\t\ta= torch.mul(edges.dst['ft_attn'], src_feat_nrom).sum(dim= 2, keepdim= True)\n",
    "\t\tsloc_sim= ((self.tao* self.cosine(src_sloc, dst_sloc)+ (1- self.tao)* self.cosine(src_tp_fea, dst_tp_fea))+ 1)/ 2\n",
    "\t\t# sloc_sim= 1\n",
    "\t\treturn {'e': (sloc_sim* a)* edges.data['e_type_attn'].unsqueeze(-1).unsqueeze(-1).repeat(1, a.shape[1], 1)}\n",
    "\n",
    "\t# 计算余弦相似性\n",
    "\tdef cosine(self, a, b):\n",
    "\t\treturn torch.mul(F.normalize(a, p= 2, dim= 2), F.normalize(b, p= 2, dim= 2)).sum(dim= 2, keepdim= True)\n",
    "\n",
    "\tdef forward(self, graph, feat, sloc, topo_fea):\n",
    "\t\twith graph.local_scope():\n",
    "\t\t\tsloc= self.feat_drop4loc(sloc.unsqueeze(1))\n",
    "\t\t\ttopo_fea= self.feat_drop4loc(topo_fea.unsqueeze(1))\n",
    "\t\t\th_src= h_dst= self.feat_drop4cont(feat)\n",
    "\t\t\t# feat_src, feat_dst, (1546, 4, 16);\n",
    "\t\t\tfeat_src= feat_dst= torch.cat([self.fc4node_proj[0](h_src[0: 1209, :]), self.fc4node_proj[1](h_src[1209: , :])], dim= 0).view(-1, self._num_heads, self._out_feats)\n",
    "\t\t\t# (1546, 4, 16) >> [(1546, 16), ..., ] >> (1546, 64) >> (1546, 4, 16); \n",
    "\t\t\tfeat_dst_attn= torch.cat([F.softmax(torch.tanh(self.fc4prob[i](feat_dst[:, i, :])), dim= 1) for i in range(self._num_heads)], dim= 1).view(-1, self._num_heads, self._out_feats)\n",
    "\t\t\tgraph.edata.update({'e_type_attn': self.etype_attn[graph.edata['e_feat']]})\n",
    "\t\t\t# ft, (1546, 4, 16)按dim= 0, L2;\n",
    "\t\t\tgraph.srcdata.update({'ft_norm': F.normalize(feat_src, p= 2, dim= 0), 'ft': feat_src, 'sloc': sloc, 'topo_fea': topo_fea})\n",
    "\t\t\t# ft_attn, (1546, 4, 16);\n",
    "\t\t\tgraph.dstdata.update({'ft_attn': feat_dst_attn, 'sloc': sloc, 'topo_fea': topo_fea})\n",
    "\t\t\t# 更新边 计算边上attention\n",
    "\t\t\tgraph.apply_edges(self.edge_attention)\n",
    "\t\t\tgraph.edata['a']= self.attn_drop(edge_softmax(graph, graph.edata.pop('e')))\n",
    "\t\t\t# 消息传递\n",
    "\t\t\tgraph.update_all(fn.u_mul_e('ft', 'a', 'm'), fn.sum('m', 'ft'))\n",
    "\t\t\tgraph.update_all(fn.u_mul_e('sloc', 'a', 'm2'), fn.sum('m2', 'sloc'))\n",
    "\t\t\tgraph.update_all(fn.u_mul_e('topo_fea', 'a', 'm3'), fn.sum('m3', 'topo_fea'))\n",
    "\t\t\t# 更新的特征\n",
    "\t\t\t# rst, (1546, 4, 16);\n",
    "\t\t\trst= graph.dstdata['ft']\n",
    "\t\t\tsloc2= graph.dstdata['sloc']\n",
    "\t\t\ttopo_fea2= graph.dstdata['topo_fea']\n",
    "\t\t\t# 残差\n",
    "\t\t\tresval= torch.cat([self.fc4res[0](h_dst[0: 1209]), self.fc4res[1](h_dst[1209:])], dim= 0).view(h_dst.shape[0], -1, self._out_feats)\n",
    "\t\t\t# (1546, 4, 64); -->> (1546, 4, 64);\n",
    "\t\t\trestult_fea= self.relu(rst+ resval).mean(1)\n",
    "\t\t\tsloc2= (sloc2+ sloc).mean(1)\n",
    "\t\t\ttopo_fea= (topo_fea2+ topo_fea).mean(1)\n",
    "\t\t\t# 返回特征\n",
    "\t\t\treturn restult_fea, sloc2, topo_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block\n",
    "class block_tf(nn.Module):\n",
    "\tdef __init__(self, node_dim= 64, edge_dim= 64, heads= 2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.node_dim, self.edge_dim, self.heads= node_dim, edge_dim, heads\n",
    "\t\t# Q, K, V , (128, 64)\n",
    "\t\t[self.Wq1, self.Wk1, self.Wv1]= [nn.Linear(self.node_dim+ self.edge_dim, self.node_dim) for i in range(3)]\n",
    "\t\t[self.Wq2, self.Wk2, self.Wv2]= [nn.Linear(self.node_dim+ self.edge_dim, self.node_dim) for i in range(3)]\t\t\n",
    "\t\t# 特征融合\n",
    "\t\tself.W1, self.W2, self.W3= nn.Linear(self.node_dim, self.node_dim), nn.Linear(self.node_dim, self.node_dim), nn.Linear(self.node_dim, self.node_dim)\n",
    "\t\tself.relu= nn.ReLU()\n",
    "\t\tself.zs= torch.zeros((4, node_dim)).to('cuda:0')\n",
    "\t\tself.layer_norm= nn.LayerNorm(self.node_dim)\n",
    "\t\t# init parameters\n",
    "\t\tself.reset_parameters()\n",
    "\n",
    "\t# 初始化权重\n",
    "\tdef reset_parameters(self):\t\n",
    "\t\tnn.init.xavier_normal_(self.Wq1.weight)\n",
    "\t\tnn.init.xavier_normal_(self.Wk1.weight)\n",
    "\t\tnn.init.xavier_normal_(self.Wv1.weight)\n",
    "\t\tnn.init.xavier_normal_(self.Wq2.weight)\n",
    "\t\tnn.init.xavier_normal_(self.Wk2.weight)\n",
    "\t\tnn.init.xavier_normal_(self.Wv2.weight)\n",
    "\t\tnn.init.xavier_normal_(self.W1.weight)\n",
    "\t\tnn.init.xavier_normal_(self.W2.weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.W3.weight)\t\t\n",
    "\n",
    "\tdef forward(self, node_mat, edge_type_mat):\n",
    "\t\tdrug_mat, micro_mat, idx0, idx1, idx2, idx3= node_mat[0: 1209], node_mat[1209:], torch.tensor([0]).cuda(), torch.tensor([1]).cuda(), torch.tensor([2]).cuda(), torch.tensor([3]).cuda()\n",
    "\t\t# 在一个关系之下, rel0, d2d, rel1, d2m, rel2, m2d, rel3, m2m;\n",
    "\t\t# h1= torch.cat((drug_mat, edge_type_mat(idx0).repeat(drug_mat.shape[0], 1)), dim= 1)\n",
    "\t\th1= torch.cat((drug_mat, self.zs[0].repeat(drug_mat.shape[0], 1)), dim= 1)\n",
    "\t\t# (1373, 64)>> (1373, heads, 64// heads)>> (heads, 1373, 64// heads)\n",
    "\t\tQ1= self.Wq1(h1).view(h1.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# (1373, 64)>> (1373, heads, 64// heads)>> (heads, 1373, 64// heads)\n",
    "\t\tK1= self.Wk1(h1).view(h1.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# (1373, 64)>> (1373, heads, 64// heads)>> (heads, 1373, 64// heads)\n",
    "\t\tV1= self.Wv1(h1).view(h1.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# h2i, h2j= torch.cat((drug_mat, edge_type_mat(idx2).repeat(drug_mat.shape[0], 1)), dim= 1), torch.cat((micro_mat, edge_type_mat(idx2).repeat(micro_mat.shape[0], 1)), dim= 1)\n",
    "\t\th2i, h2j= torch.cat((drug_mat, self.zs[1].repeat(drug_mat.shape[0], 1)), dim= 1), torch.cat((micro_mat, self.zs[2].repeat(micro_mat.shape[0], 1)), dim= 1)\t\t\n",
    "\t\t# (heads, 1373, 64// heads)\n",
    "\t\tQ2= self.Wq1(h2i).view(h2i.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# (heads, 173, 64// heads)\n",
    "\t\tK2= self.Wk2(h2j).view(h2j.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# (heads, 173, 64// heads)\n",
    "\t\tV2= self.Wv2(h2j).view(h2j.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\t# (heads, 1373, 64// heads), (heads, 1373, 64// heads)>> (4, 1373, 1373); (heads, 1373, 64// heads), (heads, 173, 64// heads)>> (4, 1373, 173); (4, 1373, 1546) \n",
    "\t\tatt4drug_kij_hat= torch.cat((torch.matmul(Q1, K1.transpose(1, 2))/ math.sqrt(Q1.shape[-1]), torch.matmul(Q2, K2.transpose(1, 2))/ math.sqrt(Q2.shape[-1])), dim= 2)\n",
    "\t\t# (4, 1373, 1546),>> (4, 1373, 1546); 按邻居数做softmax\n",
    "\t\tatt4drug_kij= F.softmax(att4drug_kij_hat, dim= 2)\n",
    "\t\t# (4, 1373, 1546), (4, 1546, 32)>> (4, 1373, 32)>> (1373, 4, 32)>> (1373, 128)\n",
    "\t\tmess1= torch.matmul(att4drug_kij, torch.cat((V1, V2), dim= 1)).transpose(0, 1).reshape(drug_mat.shape[0], -1)\n",
    "\t\t# 融合, (1373, 128)\n",
    "\t\tcofusion4drug= self.layer_norm(self.W1(mess1)+ drug_mat)\n",
    "\t\t# 获得新药物特征\n",
    "\t\tdrug_mat_new= self.layer_norm(self.W3(self.relu(self.W2(cofusion4drug)))+ cofusion4drug)\n",
    "\t\t# \n",
    "\t\t# h3i, h3j= torch.cat((micro_mat, edge_type_mat(idx1).repeat(micro_mat.shape[0], 1)), dim= 1), torch.cat((drug_mat, edge_type_mat(idx1).repeat(drug_mat.shape[0], 1)), dim= 1)\n",
    "\t\th3i, h3j= torch.cat((micro_mat, self.zs[1].repeat(micro_mat.shape[0], 1)), dim= 1), torch.cat((drug_mat, self.zs[1].repeat(drug_mat.shape[0], 1)), dim= 1)\t\t\n",
    "\t\tQ3= self.Wq2(h3i).view(h3i.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\tK3= self.Wk1(h3j).view(h3j.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\tV3= self.Wv1(h3j).view(h3j.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\t\t\n",
    "\t\t# h4= torch.cat((micro_mat, edge_type_mat(idx3).repeat(micro_mat.shape[0], 1)), dim= 1)\n",
    "\t\th4= torch.cat((micro_mat, self.zs[3].repeat(micro_mat.shape[0], 1)), dim= 1)\t\t\n",
    "\t\tQ4= self.Wq2(h4).view(h4.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\tK4= self.Wk2(h4).view(h4.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\n",
    "\t\tV4= self.Wv2(h4).view(h4.shape[0], self.heads, (self.node_dim)// self.heads).transpose(0, 1)\t\t\t\t\n",
    "\t\tatt4micro_kij_hat= torch.cat((torch.matmul(Q3, K3.transpose(1, 2))/ math.sqrt(Q3.shape[-1]), torch.matmul(Q4, K4.transpose(1, 2))/ math.sqrt(Q4.shape[-1])), dim= 2)\n",
    "\t\tatt4micro_kij= F.softmax(att4micro_kij_hat, dim= 2)\n",
    "\t\tmess2= torch.matmul(att4micro_kij, torch.cat((V3, V4), dim= 1)).transpose(0, 1).reshape(micro_mat.shape[0], -1)\n",
    "\t\tcofusion4micro= self.layer_norm(self.W1(mess2)+ micro_mat)\n",
    "\t\tmicro_mat_new= self.layer_norm(self.W3(self.relu(self.W2(cofusion4micro)))+ cofusion4micro)\n",
    "\t\t# (1546, 64)\n",
    "\t\treturn torch.cat((drug_mat_new, micro_mat_new), dim= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, in_dim1, in_dim2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.seq41= nn.Sequential(nn.Linear(in_dim1* 2, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.5))\n",
    "\t\tself.seq42= nn.Sequential(nn.Linear(in_dim2* 2, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 64), nn.ReLU(), nn.Dropout(0.5))\n",
    "\t\tself.seq4ori= nn.Sequential(nn.Linear(1381* 2, 512), nn.ReLU(), nn.Dropout(0.5), nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.5))\n",
    "\t\tself.seq4out= nn.Sequential(nn.Linear(448, 64), nn.ReLU(), nn.Dropout(0.5), nn.Linear(64, 2))\n",
    "\t\tself.reset_para()\n",
    "\t\n",
    "\tdef reset_para(self):\n",
    "\t\tfor mode in self.seq41:\n",
    "\t\t\tif isinstance(mode, nn.Linear):\n",
    "\t\t\t\tnn.init.xavier_normal_(mode.weight, gain= nn.init.calculate_gain('relu'))\n",
    "\t\tfor mode in self.seq42:\n",
    "\t\t\tif isinstance(mode, nn.Linear):\n",
    "\t\t\t\tnn.init.xavier_normal_(mode.weight, gain= nn.init.calculate_gain('relu'))\n",
    "\t\tfor mode in self.seq4ori:\n",
    "\t\t\tif isinstance(mode, nn.Linear):\n",
    "\t\t\t\tnn.init.xavier_normal_(mode.weight, gain= nn.init.calculate_gain('relu'))\t\t\t\t\n",
    "\t\tnn.init.xavier_normal_(self.seq4out[0].weight, gain= nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.seq4out[3].weight)\n",
    "\n",
    "\tdef forward(self, left_emb1, right_emb1, left_emb2, right_emb2):\n",
    "\t\tleft_ori, right_ori= left_emb1[:, 0: 1381]+ left_emb2[:, 0: 1381], right_emb1[:, 0: 1381]+ right_emb2[:, 0: 1381]\n",
    "\t\tori_out= self.seq4ori(torch.cat((left_ori, right_ori), dim= 1))\n",
    "\t\t# print(out1.shape)\n",
    "\t\tout1, out2= self.seq41(torch.cat((left_emb1[:, 1381:], right_emb1[:, 1381: ]), dim= 1)), self.seq42(torch.cat((left_emb2[:, 1381: ], right_emb2[:, 1381: ]), dim= 1))\n",
    "\t\treturn self.seq4out(torch.cat((ori_out, out1, out2), dim= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble heterogeneous model for microbe and drug associate prediction.\n",
    "class EHGN4MDA(nn.Module):\n",
    "\n",
    "\tdef __init__(self, g, features_list, e_feat, hd4gat, hd4tf, num_layers, head4gat, head4tf, feat_drop, attn_drop, k, decoder= 'mlp'):\n",
    "\t\tsuper(EHGN4MDA, self).__init__()\n",
    "\t\tself.g= g\n",
    "\t\tself.g.edata['e_feat']= e_feat\n",
    "\t\tself.features_list= features_list\n",
    "\t\tself.gat_layers= nn.ModuleList()\n",
    "\t\tself.num_layers= num_layers\n",
    "\t\t# AE 4 GAT\n",
    "\t\tself.encoder4sloc= nn.Sequential(nn.Linear(1381, 512), nn.ReLU(), nn.Linear(512, hd4gat), nn.ReLU())\n",
    "\t\tself.decoder4sloc= nn.Sequential(nn.Linear(hd4gat, 512), nn.ReLU(), nn.Linear(512, 1381), nn.Sigmoid())\t\t\t\n",
    "\t\tself.encoder4micro= nn.Sequential(nn.Linear(172, hd4gat), nn.ReLU())\n",
    "\t\tself.decoder4micro= nn.Sequential(nn.Linear(hd4gat, 172), nn.Sigmoid())\n",
    "\t\tself.encoder4drug= nn.Sequential(nn.Linear(1209, 512), nn.ReLU(), nn.Linear(512, hd4gat), nn.ReLU())\n",
    "\t\tself.decoder4drug= nn.Sequential(nn.Linear(hd4gat, 512), nn.ReLU(), nn.Linear(512, 1209), nn.Sigmoid())\n",
    "\t\tself.decoder4classify= nn.Linear(hd4gat, 2)\n",
    "\t\t# AE 4 transformer\n",
    "\t\tself.encoder4drug2= nn.Sequential(nn.Linear(1381, 512), nn.ReLU(), nn.Linear(512, hd4tf), nn.ReLU())\n",
    "\t\tself.encoder4micro2= nn.Sequential(nn.Linear(1381, 512), nn.ReLU(), nn.Linear(512, hd4tf), nn.ReLU())\n",
    "\t\tself.decoder4drug2= nn.Sequential(nn.Linear(hd4tf, 512), nn.ReLU(), nn.Linear(512, 1381), nn.Sigmoid())\t\t\t\n",
    "\t\tself.decoder4micro2= nn.Sequential(nn.Linear(hd4tf, 512), nn.ReLU(), nn.Linear(512, 1381), nn.Sigmoid())\n",
    "\t\tself.decoder4classify2= nn.Linear(hd4tf, 2)\n",
    "\t\t# 边类型嵌入\n",
    "\t\tself.edge_type_emb= nn.Embedding(4, hd4tf)\n",
    "\t\tself.edge_type_emb.weight= nn.Parameter(torch.zeros((4, hd4tf)).to(torch.float))\n",
    "\t\t# self.edge_type_emb= nn.Parameter(torch.zeros((4, hd4tf)).to(torch.float))\n",
    "\t\t# transformer layers\n",
    "\t\tself.tf_layers= nn.ModuleList([block_tf(node_dim= hd4tf, edge_dim= hd4tf, heads= head4tf) for i in range(num_layers)])\n",
    "\t\t# gat layers\n",
    "\t\tfor l in range(0, num_layers):\n",
    "\t\t\tself.gat_layers.append(HGNConv(hd4gat, hd4gat, head4gat, feat_drop, attn_drop))\n",
    "\t\t# Decoder layers\n",
    "\t\tself.decoder= Decoder((hd4gat* 2+ 2)* (num_layers+ 1), (hd4tf)* (num_layers+ 1))\n",
    "\t\t# self.decoder= Decoder4Path(1546+ (hd4tf)* (num_layers+ 1))\n",
    "\t\t# init parameters\n",
    "\t\tself.reset_parameters()\n",
    "\n",
    "\t# 初始化权重\n",
    "\tdef reset_parameters(self):\n",
    "\t\t# encoder& decoder4 gat\n",
    "\t\tnn.init.xavier_normal_(self.encoder4sloc[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4sloc[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4sloc[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4sloc[2].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4drug[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4drug[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4drug[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4drug[2].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4micro[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4micro[0].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4classify.weight)\n",
    "\t\t# encoder& decoder4 transformer\n",
    "\t\tnn.init.xavier_normal_(self.encoder4drug2[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4drug2[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4drug2[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4drug2[2].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4micro2[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.encoder4micro2[2].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4micro2[0].weight, nn.init.calculate_gain('relu'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4micro2[2].weight, nn.init.calculate_gain('sigmoid'))\n",
    "\t\tnn.init.xavier_normal_(self.decoder4classify2.weight)\t\t\n",
    "\n",
    "\tdef forward(self, left, right, topo_fea, loss_fun2, loss_fun1):\n",
    "\n",
    "\t\t# 对位置信息进行编码\n",
    "\t\tass_mat= torch.cat((torch.cat((self.features_list[2], self.features_list[-1]), dim= 1), torch.cat((self.features_list[-1].T, self.features_list[3]), dim= 1)), dim= 0).to(torch.float)\n",
    "\t\tass_mat2= torch.cat((torch.cat((self.features_list[0], self.features_list[-1]), dim= 1), torch.cat((self.features_list[-1].T, self.features_list[1]), dim= 1)), dim= 0).to(torch.float)\n",
    "\t\tsloc= self.encoder4sloc(ass_mat)\n",
    "\t\tsloc0= sloc.clone()\n",
    "\t\t# 编码并形成特征用于Net1\n",
    "\t\tdrug_emb, micro_emb= self.encoder4drug(self.features_list[0]), self.encoder4micro(self.features_list[1])\t\t\n",
    "\t\th1= torch.cat((drug_emb, micro_emb), dim= 0)\n",
    "\t\t# 编码并形成特征用于Net2\n",
    "\t\tdrug_emb2, micro_emb2= self.encoder4drug2(ass_mat2[0: 1209]), self.encoder4micro2(ass_mat2[1209:])\n",
    "\t\th2= torch.cat((drug_emb2, micro_emb2), dim= 0)\n",
    "\t\temb1, emb2= [], []\n",
    "\t\t# transformer模块\n",
    "\t\temb2.append(torch.cat((ass_mat2, h2), dim= 1))\n",
    "\t\tfor tf_layer in self.tf_layers:\n",
    "\t\t\th2= tf_layer(h2, self.edge_type_emb)\n",
    "\t\t\temb2.append(h2)\n",
    "\t\t# GAT模块, (64+ 64+ 3)* 3\n",
    "\t\temb1.append(torch.cat((ass_mat, h1, sloc, topo_fea), dim= 1))\n",
    "\t\tfor l in range(self.num_layers):\n",
    "\t\t\th1, sloc, topo_fea= self.gat_layers[l](self.g, h1, sloc, topo_fea)\n",
    "\t\t\temb1.append(torch.cat((h1, sloc, topo_fea), dim= 1))\n",
    "\t\temb1, emb2= torch.cat(emb1, dim= 1), torch.cat(emb2, dim= 1)\n",
    "\t\tbatch_label0, right4micro= torch.zeros(1381).to(torch.long).cuda(), right- 1209\n",
    "\t\tloss4gat_recon= loss_fun2(self.decoder4drug(drug_emb[left]), self.features_list[0][left])+ loss_fun2(self.decoder4micro(micro_emb[right4micro]), self.features_list[1][right4micro]) + loss_fun2(self.decoder4sloc(sloc0[left]), ass_mat[left])+ loss_fun2(self.decoder4sloc(sloc0[right]), ass_mat[right])\n",
    "\t\tloss4gat_classify= loss_fun1(self.decoder4classify(drug_emb[left]), batch_label0[left])+ loss_fun1(self.decoder4classify(micro_emb[right4micro]), batch_label0[right4micro]+ 1)\n",
    "\t\tloss4transformer_recon= loss_fun2(self.decoder4drug2(drug_emb2[left]), ass_mat2[left])+ loss_fun2(self.decoder4micro2(micro_emb2[right4micro]), ass_mat2[right])\n",
    "\t\tloss4transformer_classify= loss_fun1(self.decoder4classify2(drug_emb2[left]), batch_label0[left])+ loss_fun1(self.decoder4classify2(micro_emb2[right4micro]), batch_label0[right]+ 1)\n",
    "\t\treturn loss4gat_recon+ loss4gat_classify+ loss4transformer_recon+ loss4transformer_classify, self.decoder(emb1[left], emb1[right], emb2[left], emb2[right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl= dataloader(params)\n",
    "train_label, valid_label, test_label= dl.drug_micr_asso_mat[dl.train_xy[:, 0], dl.train_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.valid_xy[:, 0], dl.valid_xy[:, 1]].long(), dl.drug_micr_asso_mat[dl.test_xy[:, 0], dl.test_xy[:, 1]].long()\n",
    "# train\n",
    "train_xy_label_dataset= torch.utils.data.TensorDataset(dl.train_xy, train_label)\n",
    "train_loader= torch.utils.data.DataLoader(train_xy_label_dataset, batch_size= params.batch_size, shuffle= True)\n",
    "# valid\n",
    "valid_xy_label_dataset= torch.utils.data.TensorDataset(dl.valid_xy, valid_label)\n",
    "valid_loader= torch.utils.data.DataLoader(valid_xy_label_dataset, batch_size= params.batch_size, shuffle= False)\n",
    "# test, \n",
    "test_xy_label_dataset= torch.utils.data.TensorDataset(dl.test_xy, test_label)\n",
    "test_loader= torch.utils.data.DataLoader(test_xy_label_dataset, batch_size= params.batch_size, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_auc_aupr_cpt(test_xy, test_label, pred, ass_mat_shape):\n",
    "    label_mat, pred_mat= torch.zeros((ass_mat_shape)) -1, torch.zeros((ass_mat_shape)) -1\n",
    "    label_mat[test_xy[:, 0], test_xy[:, 1]], pred_mat[test_xy[:, 0], test_xy[:, 1]]= test_label* 1.0, pred\n",
    "    bool_mat4mark_test_examp= (label_mat!= -1)\n",
    "    aucs, auprs= [], []\n",
    "    for i in range(ass_mat_shape[0]):\n",
    "        test_examp_loc= bool_mat4mark_test_examp[i]\n",
    "        pos_num= label_mat[i, test_examp_loc].sum()\n",
    "        if pos_num> 0 and (test_examp_loc).sum()- pos_num> 0:\n",
    "            fpr4rowi, tpr4rowi, _= roc_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi, recall4rowi, _= precision_recall_curve(label_mat[i, test_examp_loc], pred_mat[i, test_examp_loc])\n",
    "            prec4rowi[-1]= [1, 0][(int)(prec4rowi[-2]== 0)]\n",
    "            aucs.append(auc(fpr4rowi, tpr4rowi));auprs.append(auc(recall4rowi, prec4rowi))\n",
    "    return np.mean(aucs), np.mean(auprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "\t\"\"\"docstring for EarlyStopping\"\"\"\n",
    "\tdef __init__(self, patience, pt_file= 'checkpoint/', file_name= 'checkpoint.pt', mess_out= True, eps= 0):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.patience, self.eps, self.pt_file, self.file_name, self.mess_out= patience, eps, pt_file, file_name, mess_out\n",
    "\t\tself.best_score, self.counter, self.flag= None, 0, False\n",
    "\t\tif os.path.exists(self.pt_file)== False:os.makedirs(self.pt_file)\n",
    "\t\n",
    "\tdef __call__(self, val_loss, model):\n",
    "\t\tscore= -val_loss\n",
    "\t\tif self.best_score is None:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\telif score< self.best_score- self.eps:\n",
    "\t\t\tself.counter+= 1\n",
    "\t\t\tif self.mess_out:print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "\t\t\tif self.counter>= self.patience:\n",
    "\t\t\t\tself.flag= True\n",
    "\t\telse:\n",
    "\t\t\tself.best_score= score\n",
    "\t\t\tself.save_checkpoint(model)\n",
    "\t\t\tself.counter= 0\n",
    "\n",
    "\tdef save_checkpoint(self, model):\n",
    "\t\ttorch.save(model, f'{self.pt_file}//{self.file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.fea_mats, dl.g, topo_fea, dl.e_feat= [item.to(torch.float).to(params.device) for item in dl.fea_mats], dl.g.to(params.device), dl.topo_fea.to(params.device), dl.e_feat.to(params.device)\n",
    "net= EHGN4MDA(dl.g, dl.fea_mats, dl.e_feat, params.hd4gat, params.hd4tf, params.layers_num, params.head4gat, params.head4transformer, params.feat_dropout, params.attn_dropout, params.wr_time, params.decoder).to(params.device)\n",
    "optimizer= torch.optim.Adam(net.parameters(), lr= params.lr, weight_decay= params.weight_decay)\n",
    "earlystopping= EarlyStopping(patience= params.patience, pt_file= params.pt_file, file_name= params.pt_file_name, mess_out= True)\n",
    "loss_func1, loss_func2= nn.CrossEntropyLoss(), nn.MSELoss()\n",
    "pred= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(params.epochs):\n",
    "    # train\n",
    "    for step, (xy, label) in enumerate(train_loader):\n",
    "        net.train()\n",
    "        time_start= time.time()\n",
    "        xy, label= xy.to(params.device), label.to(params.device)\n",
    "        train_loss2, logp= net(xy[:, 0], xy[:, 1]+ 1209, topo_fea, loss_func2, loss_func1)\n",
    "        train_loss1= loss_func1(logp, label)\n",
    "        loss= 0.8* train_loss1+  0.2* train_loss2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        time_end= time.time()\n",
    "        if step%500== 0:\n",
    "            print(f'epoch: {ep+ 1}, step: {step+ 1}, train loss: {loss}, time: {time_end- time_start}')\n",
    "    # valid\n",
    "    net.eval(); val_loss= 0; pred= []\n",
    "    with torch.no_grad():\n",
    "        for step, (xy, label) in enumerate(valid_loader):\n",
    "            xy, label= xy.to(params.device), label.to(params.device)\n",
    "            valid_loss2, logp= net(xy[:, 0], xy[:, 1]+ 1209, topo_fea, loss_func2, loss_func1)\n",
    "            valid_loss1= loss_func1(logp, label)\n",
    "            val_loss= 0.8* valid_loss1+ 0.2* valid_loss2\n",
    "            pred.append(logp)\n",
    "    pred= torch.cat(pred).cpu()\n",
    "    roc_auc, aupr_auc= avg_auc_aupr_cpt(dl.valid_xy, valid_label, pred[:, 1], (1209, 172))\n",
    "    print(f'epoch: {ep+ 1}, valid loss: {val_loss}, auc: {roc_auc}, aupr: {aupr_auc}')\n",
    "    earlystopping(-(roc_auc+ aupr_auc), net)\n",
    "    if earlystopping.flag== True:print(f'early_stopping');break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net= torch.load(f'{params.pt_file}//{params.pt_file_name}')\n",
    "net.eval();results= []\n",
    "with torch.no_grad():\n",
    "    for _, (xy, _) in enumerate(test_loader):\n",
    "        _, logp= net(xy[:, 0], xy[:, 1]+ 1209, topo_fea, loss_func2, loss_func1)\n",
    "        results.append(logp)\n",
    "pred= torch.cat(results, dim= 0)\n",
    "results= torch.cat([dl.test_xy.to('cpu'), test_label.view(-1, 1).to('cpu'), pred[:, 1].view(-1, 1).to('cpu')], dim= 1)\n",
    "print(avg_auc_aupr_cpt(dl.test_xy.to('cpu'), test_label.to('cpu'), pred[:, 1].to('cpu'), (1209, 172)))\n",
    "# np.savetxt(fname= params.test_result_file, X= results, delimiter= '\\t', encoding= 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
